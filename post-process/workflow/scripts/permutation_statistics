#!/usr/bin/env python

import pandas as pd
import argparse
from loguru import logger
import sys
import os
from ete3 import Tree
import seaborn as sns

def permute_and_compute(tree, clusters):
    random.shuffle(clusters)
    for node, cluster in zip(tree.iter_leaves(), clusters):
        node.cluster = cluster
    return compute_statistic(tree)


def main():
    parser = argparse.ArgumentParser(description="Permutation test of leaf labels for a given phylogeny")
    parser.add_argument(
        "-i",
        "--inputdir",
        type=str,
        required=True,
        help="Input CSV output for a passing gene",
    )
    parser.add_argument(
        "-f",
        "--inputfof",
        type=str,
        required=True,
        help="Input batch file",
    )
    parser.add_argument(
        "-m",
        "--maxgenomes",
        type=int,
        default=1000,
        help="Max. number of genomes to decompress",
    )
    parser.add_argument(
        "-e",
        "--extraction_dir",
        type=str,
        required=True,
        help="Extraction directory",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        required=True,
        help=
        "Path to the output downsampled dataframe that contains the paths to the extracted cluster reps."
    )
    parser.add_argument(
        "-q",
        "--metadata",
        type=str,
        help="QC metadata file for 661K collection",
        required=False,
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    os.makedirs(args.extraction_dir, exist_ok=True)

    if args.verbose == 0:
        logger.remove()
    elif args.verbose == 1:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    elif args.verbose >= 2:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    logger.info("Reading input cluster csv")

    df = pd.read_csv(args.inputdir)
    file_mapping = {}
    with open(args.inputfof, "r") as f:
        for line in f:
            bucket_name = os.path.basename(line.strip()).split(".tar.xz")[0]
            file_mapping[bucket_name] = str(line.strip())

    if args.metadata:
        df = df.loc[df['GenomeID'].isin(get_hq(args.metadata))]

    downsampled_df = sample_clusters(df, args.maxgenomes)

    grouped = downsampled_df.groupby("Bucket")
    with multiprocessing.Pool() as pool:
        pool.map(
            process_bucket_group_wrapper,
            [(group, file_mapping, args.extraction_dir)
             for _, group in grouped],
        )
    downsampled_df.to_csv(args.output)
    logger.success("Extracted all files successfully")
