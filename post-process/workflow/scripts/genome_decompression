#!/usr/bin/env python

import pandas as pd
import multiprocessing
import argparse
import glob
import tarfile
from loguru import logger
import sys
import os
import subprocess
import concurrent.futures


def get_hq(hq_file):
    metadata_df = pd.read_csv(hq_file, sep='\t')
    hq_samples = metadata_df.loc[metadata_df['high_quality'] ==
                                 True]['sample_id']
    return hq_samples


def sample_clusters(df, maxgenomes):
    if maxgenomes >= len(df):
        logger.warning(
            "Warning: maxgenomes is greater than the number of available genomes. Returning the entire dataframe."
        )
        sampled_df = df.copy()

    # Step 1: Find unique clusters and their counts
    # unique_clusters = df.loc[df['Cluster'] != '-1']['Cluster'].value_counts()
    unique_clusters = df['Cluster'].value_counts()
    logger.info(f"{len(unique_clusters)} unique clusters exist")

    # Step 2: Calculate the ideal number of samples per cluster
    ideal_samples_per_cluster = maxgenomes // len(unique_clusters)
    logger.info(f"{ideal_samples_per_cluster} samples per cluster")

    # Step 3: Sample from each cluster
    sampled_df = pd.DataFrame()
    for cluster, count in unique_clusters.items():
        if count > ideal_samples_per_cluster:
            sampled_df = pd.concat([
                sampled_df,
                df[df['Cluster'] == cluster].sample(ideal_samples_per_cluster,
                                                    random_state=1)
            ])
        else:
            sampled_df = pd.concat([sampled_df, df[df['Cluster'] == cluster]])

    # Step 4: Distribute remaining quota if maxgenomes is not reached
    remaining_genomes = maxgenomes - len(sampled_df)
    if remaining_genomes > 0:
        excess_clusters = df[~df.index.isin(sampled_df.index)]
        additional_samples = excess_clusters.sample(remaining_genomes)
        sampled_df = pd.concat([sampled_df, additional_samples])
    return sampled_df


def process_bucket(group, file_mapping, extraction_dir):
    bucket = group["Bucket"].iloc[0]
    archive_file = file_mapping.get(bucket)

    if not archive_file:
        logger.warning(f"No archive file found for Bucket: {bucket}")
        return

    # Create a set of unique genome filenames to decompress
    genome_set = set(group["GenomeID"])

    # Ensure the extraction directory exists
    if not os.path.exists(extraction_dir):
        os.makedirs(extraction_dir)

    # Process the tar.xz archive
    try:
        with tarfile.open(archive_file, "r:xz") as tar:
            for member in tar.getmembers():
                # Check if the current file is one of the desired genomes
                if any(genome_id in member.name for genome_id in genome_set):
                    # Extract only the matching file
                    member.name = os.path.basename(member.name)
                    tar.extract(member, path=extraction_dir)
                    logger.info(f"Extracted {member.name} to {extraction_dir}")
    except Exception as e:
        logger.error(f"Error processing archive {archive_file}: {e}")


def process_bucket_group_wrapper(args):
    group, file_mapping, extraction_dir = args
    return process_bucket(group, file_mapping, extraction_dir)


def main():
    parser = argparse.ArgumentParser(description="Gene clustering script.")
    parser.add_argument(
        "-i",
        "--inputdir",
        type=str,
        required=True,
        help="Input CSV output for a passing gene",
    )
    parser.add_argument(
        "-f",
        "--inputfof",
        type=str,
        required=True,
        help="Input batch file",
    )
    parser.add_argument(
        "-m",
        "--maxgenomes",
        type=int,
        default=1000,
        help="Max. number of genomes to decompress",
    )
    parser.add_argument(
        "-e",
        "--extraction_dir",
        type=str,
        required=True,
        help="Extraction directory",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        required=True,
        help=
        "Path to the output downsampled dataframe that contains the paths to the extracted cluster reps."
    )
    parser.add_argument(
        "-q",
        "--metadata",
        type=str,
        help="QC metadata file for 661K collection",
        required=False,
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    os.makedirs(args.extraction_dir, exist_ok=True)

    if args.verbose == 0:
        logger.remove()
    elif args.verbose == 1:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    elif args.verbose >= 2:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    logger.info("Reading input cluster csv")

    df = pd.read_csv(args.inputdir)
    file_mapping = {}
    with open(args.inputfof, "r") as f:
        for line in f:
            bucket_name = os.path.basename(line.strip()).split(".tar.xz")[0]
            file_mapping[bucket_name] = str(line.strip())

    if args.metadata:
        df = df.loc[df['GenomeID'].isin(get_hq(args.metadata))]

    downsampled_df = sample_clusters(df, args.maxgenomes)

    grouped = downsampled_df.groupby("Bucket")
    with multiprocessing.Pool() as pool:
        pool.map(
            process_bucket_group_wrapper,
            [(group, file_mapping, args.extraction_dir)
             for _, group in grouped],
        )
    downsampled_df.to_csv(args.output)
    logger.success("Extracted all files successfully")


if __name__ == "__main__":
    main()
