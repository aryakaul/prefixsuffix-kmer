#!/usr/bin/env python

import pandas as pd
import multiprocessing
import argparse
import glob
import tarfile
from loguru import logger
import sys
import os
import subprocess
import concurrent.futures


def get_hq(hq_file):
    metadata_df = pd.read_csv(hq_file, sep='\t')
    hq_samples = metadata_df.loc[metadata_df['high_quality'] ==
                                 True]['sample_id']
    return hq_samples


def sample_clusters(df, maxgenomes):
    if maxgenomes >= len(df):
        logger.warning(
            "Warning: maxgenomes is greater than the number of available genomes. Returning the entire dataframe."
        )
        sampled_df = df.copy()

    # Step 1: Find unique clusters and their counts
    # unique_clusters = df.loc[df['Cluster'] != '-1']['Cluster'].value_counts()
    unique_clusters = df['Cluster'].value_counts()
    logger.info(f"{len(unique_clusters)} unique clusters exist")

    # Step 2: Calculate the ideal number of samples per cluster
    ideal_samples_per_cluster = maxgenomes // len(unique_clusters)
    logger.info(f"{ideal_samples_per_cluster} samples per cluster")

    # Step 3: Sample from each cluster
    sampled_df = pd.DataFrame()
    for cluster, count in unique_clusters.items():
        if count > ideal_samples_per_cluster:
            sampled_df = pd.concat([
                sampled_df,
                df[df['Cluster'] == cluster].sample(ideal_samples_per_cluster,
                                                    random_state=1)
            ])
        else:
            sampled_df = pd.concat([sampled_df, df[df['Cluster'] == cluster]])

    # Step 4: Distribute remaining quota if maxgenomes is not reached
    remaining_genomes = maxgenomes - len(sampled_df)
    if remaining_genomes > 0:
        excess_clusters = df[~df.index.isin(sampled_df.index)]
        additional_samples = excess_clusters.sample(remaining_genomes)
        sampled_df = pd.concat([sampled_df, additional_samples])
    return sampled_df


def process_bucket(group, file_mapping, extraction_dir):
    bucket = group["Bucket"].iloc[0]
    archive_file = file_mapping.get(bucket)

    if not archive_file:
        logger.warning(f"No archive file found for Bucket: {bucket}")
        return

    # Create a set of unique genome filenames to decompress
    genome_set = set(group["GenomeID"])

    # Ensure the extraction directory exists
    if not os.path.exists(extraction_dir):
        os.makedirs(extraction_dir)

    # Process the tar.xz archive
    try:
        with tarfile.open(archive_file, "r:xz") as tar:
            for member in tar.getmembers():
                # Check if the current file is one of the desired genomes
                if any(genome_id in member.name for genome_id in genome_set):
                    # Extract only the matching file
                    member.name = os.path.basename(member.name)
                    tar.extract(member, path=extraction_dir)
                    logger.info(f"Extracted {member.name} to {extraction_dir}")
    except Exception as e:
        logger.error(f"Error processing archive {archive_file}: {e}")


def process_bucket_group_wrapper(args):
    group, file_mapping, extraction_dir = args
    return process_bucket(group, file_mapping, extraction_dir)

def read_csv(input_dir):
    print(input_dir)
    search = glob.glob(os.path.join(input_dir, "*/*_clusters.csv"))
    print(search)
    dataframes = []
    for x in search:
        df = pd.read_csv(x)
        dataframes.append(df)
    return pd.concat(dataframes, ignore_index=True)


def sample_clusters(df, samples):
    sampled_df = df.loc[df['Cluster'] != -1].groupby(['GeneID', 'Cluster'
                                                      ]).sample(n=samples,
                                                                random_state=0)
    return sampled_df


def python_grep(file_path, search_string):
    count = 0
    hits = []
    with open(file_path, 'r') as file:
        for line in file:
            if search_string in line:
                hits.append(line.rstrip())
                count += 1
    if count > 1:
        logger.warning(
            f"Multiple results found for {search_string} in {file_path}...")
        logger.warning(f"Hits: {hits}")
    return hits[0]


def extract_specific_file(tar_path, file_name, extract_to):
    """
    Extracts a specific file from a tar.xz archive using the tar command.
    """

    target_path = os.path.join(
        os.path.basename(tar_path).split('.tar.xz')[0], file_name)

    batch_outdir = os.path.join(extract_to,
                                os.path.basename(tar_path).split('.tar.xz')[0])
    full_extract_path = os.path.join(batch_outdir, file_name)
    if os.path.exists(full_extract_path):
        logger.debug(
            f"File {file_name} already exists in {batch_outdir}, skipping!")
        return

    try:
        # Construct the tar command
        logger.debug(
            f"Extracting {file_name} from {tar_path} to {batch_outdir}")
        os.makedirs(batch_outdir, exist_ok=True)
        cmd = ['tar', '-xf', tar_path, '-C', extract_to, target_path]

        # Execute the command
        subprocess.run(cmd, check=True)
        logger.info(f"Successfully extracted {file_name} from {tar_path}")

    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to extract {file_name} from {tar_path}: {e}")


def process_row(args):
    """
    Function to process each row of the DataFrame.
    """
    row, extraction_directory = args
    tar_path = row.TarPath
    genome_id = row.GenomeID + '.fa'
    extract_specific_file(tar_path, genome_id, extraction_directory)


def extract_all_files(df, ext_dir):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        args = ((row, ext_dir)
                for row in df.drop_duplicates('TarPath').itertuples(
                    index=False))
        results = executor.map(process_row, args)
        for result in results:
            pass


def main():
    parser = argparse.ArgumentParser(description="Gene clustering script.")
    parser.add_argument(
        "-i",
        "--inputdir",
        type=str,
        required=True,
        help="Input CSV output for a passing gene",
    )
    parser.add_argument(
        "-f",
        "--inputfof",
        type=str,
        required=True,
        help="Input batch file",
    )
    parser.add_argument(
        "-m",
        "--maxgenomes",
        type=int,
        default=1000,
        help="Max. number of genomes to decompress",
    )
    parser.add_argument(
        "-e",
        "--extraction_dir",
        type=str,
        required=True,
        help="Extraction directory",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        required=True,
        help=
        "Path to the output downsampled dataframe that contains the paths to the extracted cluster reps."
    )
    parser.add_argument(
        "-q",
        "--metadata",
        type=str,
        help="QC metadata file for 661K collection",
        required=False,
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    os.makedirs(args.extraction_dir, exist_ok=True)

    if args.verbose == 0:
        logger.remove()
    elif args.verbose == 1:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    elif args.verbose >= 2:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    logger.info("Reading input cluster csv")

    df = pd.read_csv(args.inputdir)
    file_mapping = {}
    with open(args.inputfof, "r") as f:
        for line in f:
            bucket_name = os.path.basename(line.strip()).split(".tar.xz")[0]
            file_mapping[bucket_name] = str(line.strip())

    if args.metadata:
        df = df.loc[df['GenomeID'].isin(get_hq(args.metadata))]

    downsampled_df = sample_clusters(df, args.maxgenomes)

    grouped = downsampled_df.groupby("Bucket")
    with multiprocessing.Pool() as pool:
        pool.map(
            process_bucket_group_wrapper,
            [(group, file_mapping, args.extraction_dir)
             for _, group in grouped],
        )
    downsampled_df.to_csv(args.output)
    logger.success("Extracted all files successfully")


if __name__ == "__main__":
    main()
