#!/usr/bin/env python

import pandas as pd
import multiprocessing
import argparse
import glob
import tarfile
from loguru import logger
import sys
import os
import subprocess
import concurrent.futures

def process_bucket(group, file_mapping, extraction_dir):
    bucket = group["Bucket"].iloc[0]
    archive_file = file_mapping.get(bucket)

    if not archive_file:
        logger.warning(f"No archive file found for Bucket: {bucket}")
        return

    # Create a set of unique genome filenames to decompress
    genome_set = set(group["GenomeID"])

    # Ensure the extraction directory exists
    if not os.path.exists(extraction_dir):
        os.makedirs(extraction_dir)

    # Process the tar.xz archive
    try:
        with tarfile.open(archive_file, "r:xz") as tar:
            for member in tar.getmembers():
                basename = os.path.basename(member.name)
                curr_genomeid, _ = os.path.splitext(basename)
                # Check if the current file is one of the desired genomes
                # if any(genome_id in member.name for genome_id in genome_set):
                if curr_genomeid in genome_set:
                    # Extract only the matching file
                    member.name = os.path.basename(member.name)
                    tar.extract(member, path=extraction_dir)
                    logger.info(f"Extracted {member.name} to {extraction_dir}")
    except Exception as e:
        logger.error(f"Error processing archive {archive_file}: {e}")


def process_bucket_group_wrapper(args):
    group, file_mapping, extraction_dir = args
    return process_bucket(group, file_mapping, extraction_dir)


def read_csv(input_dir):
    search = glob.glob(os.path.join(input_dir, "*/*_clusters.csv"))
    dataframes = []
    for x in search:
        df = pd.read_csv(x)
        dataframes.append(df)
    return pd.concat(dataframes, ignore_index=True)


def main():
    parser = argparse.ArgumentParser(description="Gene clustering script.")
    parser.add_argument(
        "-i",
        "--inputcsv",
        type=str,
        required=True,
        help="Input CSV output for a passing gene",
    )
    parser.add_argument(
        "-f",
        "--inputfof",
        type=str,
        required=True,
        help="Input batch file",
    )
    parser.add_argument(
        "-e",
        "--extraction_dir",
        type=str,
        required=True,
        help="Extraction directory",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    os.makedirs(args.extraction_dir, exist_ok=True)

    if args.verbose == 0:
        logger.remove()
    elif args.verbose == 1:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    elif args.verbose >= 2:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    logger.info("Reading input cluster csv")

    df = pd.read_csv(args.inputcsv)
    file_mapping = {}
    with open(args.inputfof, "r") as f:
        for line in f:
            bucket_name = os.path.basename(line.strip()).split(".tar.xz")[0]
            file_mapping[bucket_name] = str(line.strip())

    df['Bucket'] = df['filename'].str.extract(r'.*/(.*)_.*-filterdists\.csv\.gz$', expand=False)
    grouped = df.groupby("Bucket")
    with multiprocessing.Pool() as pool:
        pool.map(
            process_bucket_group_wrapper,
            [(group, file_mapping, args.extraction_dir)
             for _, group in grouped],
        )
    logger.success("Extracted all files successfully")


if __name__ == "__main__":
    main()
