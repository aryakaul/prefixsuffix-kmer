#!/usr/bin/env python

import pandas as pd
import argparse
from loguru import logger
import sys
import os
import subprocess
import concurrent.futures


def read_csv(input_csv):
    return pd.read_csv(input_csv)


def sample_clusters(df, samples):
    sampled_df = df.loc[df['Cluster'] != -1].groupby(['GeneID', 'Cluster'
                                                      ]).sample(n=samples,
                                                                random_state=0)
    return sampled_df


def python_grep(file_path, search_string):
    count = 0
    hits = []
    with open(file_path, 'r') as file:
        for line in file:
            if search_string in line:
                hits.append(line.rstrip())
                count += 1
    if count > 1:
        logger.warning(
            f"Multiple results found for {search_string} in {file_path}...")
        logger.warning(f"Hits: {hits}")
    return hits[0]


def extract_specific_file(tar_path, file_name, extract_to):
    """
    Extracts a specific file from a tar.xz archive using the tar command.
    """

    target_path = os.path.join(
        os.path.basename(tar_path).split('.tar.xz')[0], file_name)

    batch_outdir = os.path.join(extract_to,
                                os.path.basename(tar_path).split('.tar.xz')[0])
    full_extract_path = os.path.join(batch_outdir, file_name)
    if os.path.exists(full_extract_path):
        logger.debug(
            f"File {file_name} already exists in {batch_outdir}, skipping!")
        return

    try:
        # Construct the tar command
        logger.debug(
            f"Extracting {file_name} from {tar_path} to {batch_outdir}")
        os.makedirs(batch_outdir, exist_ok=True)
        cmd = ['tar', '-xf', tar_path, '-C', extract_to, target_path]

        # Execute the command
        subprocess.run(cmd, check=True)
        logger.info(f"Successfully extracted {file_name} from {tar_path}")

    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to extract {file_name} from {tar_path}: {e}")


def process_row(args):
    """
    Function to process each row of the DataFrame.
    """
    row, extraction_directory = args
    tar_path = row.TarPath
    genome_id = row.GenomeID + '.fa'
    extract_specific_file(tar_path, genome_id, extraction_directory)


def extract_all_files(df, ext_dir):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        args = ((row, ext_dir) for row in df.itertuples(index=False))
        results = executor.map(process_row, args)
        for result in results:
            pass


def main():
    parser = argparse.ArgumentParser(description="Gene clustering script.")
    parser.add_argument(
        "-i",
        "--input_csv",
        type=str,
        required=True,
        help="Input CSV output for a passing gene",
    )
    parser.add_argument(
        "-f",
        "--inputfile",
        type=str,
        required=True,
        help="Input batch file",
    )
    parser.add_argument(
        "-s",
        "--samples",
        type=int,
        default=4,
        help="Number of genomes to select from each cluster found",
    )
    parser.add_argument(
        "-e",
        "--extraction_dir",
        type=str,
        required=True,
        help="Extraction directory",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        required=True,
        help=
        "Path to the output downsampled dataframe that contains the paths to the extracted cluster reps."
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    if args.verbose == 0:
        logger.remove()
    elif args.verbose == 1:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    elif args.verbose >= 2:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    logger.info("Reading input cluster csv")
    df = read_csv(args.input_csv)
    downsampled_df = sample_clusters(df, args.samples)
    downsampled_df['TarPath'] = downsampled_df['Bucket'].apply(
        lambda x: python_grep(args.inputfile, x + '.tar.xz'))
    logger.info("Added paths to all tar files")
    os.makedirs(args.extraction_dir, exist_ok=True)
    extract_all_files(downsampled_df, args.extraction_dir)
    # downsampled_df['FastaPath'] = os.path.join(args.extraction_dir, downsampled_df['Bucket'], downsampled_df['GenomeID']+".fa")
    downsampled_df['FastaPath'] = downsampled_df.apply(
        lambda row: os.path.join(args.extraction_dir, row['Bucket'], row[
            'GenomeID'] + ".fa"),
        axis=1)
    downsampled_df.to_csv(args.output)
    logger.success("Extracted all files successfully")


if __name__ == "__main__":
    main()
