#!/usr/bin/env python

import duckdb
import scipy
from loguru import logger
import matplotlib.pyplot as plt
import os
import re
import uuid
from dbscan1d.core import DBSCAN1D
import seaborn as sns
from matplotlib.collections import PathCollection
from multiprocessing import Pool
import glob
import pandas as pd
import sys
import argparse
import tempfile
import numpy as np
import concurrent.futures
import matplotlib.ticker as ticker


def process_gene(args):
    gene_id, epsilon, min_samples, output_dir, remove_outliers, db_file = args

    # Set up logging in the child process
    logger.remove()
    logger.add(sys.stderr, level="INFO")

    # Connect to the persistent DuckDB database
    con = duckdb.connect(database=db_file, read_only=True)

    # Use a parameterized query to safely include gene_id
    query = "SELECT * FROM gene_data WHERE GeneID = ?"
    df = con.execute(query, [gene_id]).df()
    con.close()

    if df.empty:
        # No data for this gene
        return gene_id, None, False, None

    df['Bucket'] = df['filename'].apply(lambda x: re.sub(r'-filterdists\.csv\.gz$', '', os.path.basename(x)))

    # Clean gene_name for file system usage
    gene_name = gene_id.replace("/", ":").replace("|", ":").replace(
        "(", ":").replace(")", ":").replace("'", ":")

    # Ensure 'Difference' column is numeric
    df['Difference'] = pd.to_numeric(df['Difference'], errors='coerce')

    # Drop rows with NaN 'Difference' values
    df = df.dropna(subset=['Difference'])

    if df.empty:
        logger.info(
            f"GeneID {gene_id} has no valid 'Difference' data after cleaning.")
        return gene_id, None, False, None

    # Count the occurrences of each unique value for sample_weight
    value_counts = df['Difference'].value_counts()

    # Remove exact duplicates
    gene_data_unique = df.drop_duplicates(subset=["Difference"])

    # Reshape for DBSCAN
    differences = gene_data_unique["Difference"].values.reshape(-1, 1)

    # Use sample_weight in DBSCAN
    sample_weight = value_counts.loc[gene_data_unique["Difference"]].values

    # Perform clustering
    dbscan = DBSCAN1D(eps=epsilon, min_samples=min_samples)
    labels = dbscan.fit_predict(differences, sample_weight=sample_weight)

    # Check if the gene meets the criteria
    label_set = set(labels)
    if (-1 in label_set and len(label_set) <= 2) or len(label_set) < 2:
        logger.info(f"GeneID {gene_id} did not pass the clustering criteria.")
        return gene_id, None, False, None

    logger.info(f"GeneID {gene_id} passed the clustering criteria.")

    # Compute mean distance for each cluster except noise (-1)
    cluster_means = {}
    for label in label_set:
        if label != -1:
            cluster_members = differences[labels == label]
            mean_distance = np.mean(cluster_members)
            cluster_means[label] = mean_distance

    # Sort clusters based on their mean distance and assign new labels
    sorted_clusters = sorted(cluster_means, key=cluster_means.get)
    cluster_mapping = {
        old_label: new_label
        for new_label, old_label in enumerate(sorted_clusters)
    }

    # Replace old labels with new labels
    new_labels = np.array(
        [cluster_mapping[label] if label != -1 else -1 for label in labels])

    # Create a mapping from unique values to labels
    mapping = dict(zip(gene_data_unique["Difference"], new_labels))

    # Add 'Cluster' column to df
    df["Cluster"] = df["Difference"].map(mapping)

    outdir = os.path.join(output_dir, "passing_genes", gene_name)
    os.makedirs(outdir, exist_ok=True)

    # Plotting and saving
    if remove_outliers:
        plot_data = df.loc[df["Cluster"] != -1]
    else:
        plot_data = df

    create_plot(
        plot_data,
        gene_name,
        os.path.join(outdir, f"{gene_name}_clusters.png"),
    )

    # Save the DataFrame to CSV
    df.to_csv(os.path.join(outdir, f"{gene_name}_clusters.csv.gz"),
              index=False,
              compression='gzip')

    return gene_id, mapping, True, gene_name


def create_plot(big_df, gene_id, output):
    # big_df = big_df.copy()
    if "Cluster" in big_df.columns:
        palette_ints = assign_cluster_colors(big_df["Cluster"].astype(int))
        big_df["Cluster"] = big_df["Cluster"].astype(str)
        palette = {}
        for i in palette_ints:
            palette[str(i)] = palette_ints[i]

    # Create a figure with two subplots (rows), sharing the same x-axis
    fig, axs = plt.subplots(2,
                            1,
                            figsize=(5, 4),
                            sharex=True,
                            gridspec_kw={"height_ratios": [3, 1]})

    # First plot (KDE) on the first subplot
    sns.histplot(data=big_df,
                 x="Difference",
                 ax=axs[0],
                 fill=False,
                 color="black")
    axs[0].set_yscale('log')
    axs[0].set_ylabel("Number of Genomes")
    axs[0].set_title(f"{gene_id}")
    # Remove x-axis label for the top plot to only have it at the bottom plot
    axs[0].set_xlabel("")
    axs[0].yaxis.set_major_formatter(
        ticker.FuncFormatter(lambda x, pos: "{:,.0f}".format(x)
                             if x == 0 else "{:,.3g}".format(x / 1000) + "K"))

    if "Cluster" in big_df.columns:
        # Second plot (Stripplot and Violinplot) on the second subplot
        sns.stripplot(
            x="Difference",
            data=big_df.sort_values("Cluster",
                                    key=lambda x: x.replace("-1", "A")),
            hue="Cluster",
            orient="h",
            ax=axs[1],
            alpha=0.8,
            palette=palette,
        )
    else:
        sns.stripplot(
            x="Difference",
            data=big_df,
            color="black",
            orient="h",
            ax=axs[1],
            alpha=0.8,
        )

    sns.violinplot(
        x="Difference",
        data=big_df,
        fill=True,
        inner=None,
        cut=0,
        color="lightgrey",
        alpha=0.8,
        split=False,
        ax=axs[1],
    )

    if "Cluster" in big_df.columns:
        handles, labels = axs[1].get_legend_handles_labels()
        fig.legend(handles,
                   labels,
                   title="Cluster",
                   bbox_transform=fig.transFigure)
        axs[1].get_legend().remove()

    # Set labels for the second plot
    axs[1].set_xlabel("Prefix-suffix distance relative to gene length")
    axs[1].xaxis.set_major_formatter(
        ticker.FuncFormatter(lambda x, pos: "{:,.0f}".format(x)
                             if x == 0 else "{:,.3g}".format(x / 1000) + "kb"))
    axs[1].set(ylabel=None)  # Remove y-axis label
    axs[1].set_yticks([])
    sns.despine()
    sns.despine(left=True, ax=axs[1])

    # Adjust layout to not overlap and save the figure
    plt.tight_layout()

    plt.savefig(output, dpi=300, transparent=True)
    plt.close()


def assign_cluster_colors(labels):
    # Sort unique labels, ensuring -1 is last
    unique_labels = sorted(set(labels), key=lambda x: (x == -1, x))
    logger.debug(f"Sorted Labels: {unique_labels}")

    # Get colors from the seaborn muted palette
    n_colors = len(unique_labels)
    logger.debug(f"Number of colors: {n_colors}")

    # Generate a color palette with enough distinct colors
    base_palette = sns.color_palette("muted", as_cmap=False).as_hex()
    if n_colors > len(base_palette):
        logger.warning(
            f"{n_colors} is more than in the seaborn palette. Adding new colors with different brightness."
        )
        extended_palette = (base_palette + sns.color_palette(
            "muted", n_colors - len(base_palette), desat=0.6).as_hex())
    else:
        extended_palette = base_palette

    colors = extended_palette[:n_colors]
    logger.debug(f"Available colors: {colors}")

    # Initialize label_color_map with regular clusters
    label_color_map = {
        label: colors[i]
        for i, label in enumerate(unique_labels) if label != -1
    }

    # If -1 is in labels, assign it the last color from the palette
    if -1 in unique_labels:
        label_color_map[-1] = colors[
            -1]  # Assigning the last color in the palette to -1

    return label_color_map


def main():
    parser = argparse.ArgumentParser(description="Gene clustering script.")
    parser.add_argument(
        "-i",
        "--input_dir",
        type=str,
        required=True,
        help="Directory containing input CSV files.",
    )
    parser.add_argument(
        "-o",
        "--output_dir",
        type=str,
        required=True,
        help="Directory to store output files.",
    )
    parser.add_argument("--epsilon",
                        type=float,
                        default=800,
                        help="Epsilon value for DBSCAN1D.")
    parser.add_argument("--min_samples",
                        type=int,
                        default=75,
                        help="Minimum samples value for DBSCAN1D.")
    parser.add_argument(
        "--remove_outliers",
        action="store_true",
        help="Whether to remove or keep outliers in plots.",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    # Set up logging
    logger.remove()
    if args.verbose == 0:
        logger.add(sys.stderr, level="WARNING")
    elif args.verbose == 1:
        logger.add(sys.stderr, level="INFO")
    else:
        logger.add(sys.stderr, level="DEBUG")

    os.makedirs(os.path.join(args.output_dir, "all_gene_dists"), exist_ok=True)
    passinggenesdir = os.path.join(args.output_dir, "passing_genes")
    os.makedirs(passinggenesdir, exist_ok=True)

    logger.info("Constructing persistent DuckDB database.")

    # Generate a unique temporary database file
    unique_id = str(uuid.uuid4())
    # temp_db_file = os.path.join(tempfile.gettempdir(),
    # f'gene_data_{unique_id}.duckdb')
    temp_db_file = os.path.join(args.output_dir,
                                f'gene_data_{unique_id}.duckdb')

    con = duckdb.connect(database=temp_db_file, read_only=False)
    csv_pattern = os.path.join(args.input_dir, "*.csv.gz")

    # Define column types
    column_types = {
        'GenomeID': 'VARCHAR',
        'GeneID': 'VARCHAR',
        'PrefixID': 'VARCHAR',
        'SuffixID': 'VARCHAR',
        'PrefixContig': 'VARCHAR',
        'SuffixContig': 'VARCHAR',
        'Contig': 'VARCHAR',
        'Distance': 'BIGINT',
        'Difference': 'DOUBLE',
        'PrefixLocation': 'BIGINT',
        'SuffixLocation': 'BIGINT'
    }

    # Read the CSV files into a relation
    rel = con.from_csv_auto(
        csv_pattern,
        compression='gzip',
        filename=True,
        dtype=column_types,
    )

    # Create or replace the table
    rel.create('gene_data')

    # Get list of unique GeneIDs
    gene_ids_query = "SELECT DISTINCT GeneID FROM gene_data"
    gene_ids = [row[0] for row in con.execute(gene_ids_query).fetchall()]
    con.close()
    logger.info(f"Found {len(gene_ids)} unique GeneIDs.")

    # Create argument tuples for each gene
    args_list = []
    for gene_id in gene_ids:
        args_list.append((gene_id, args.epsilon, args.min_samples,
                          args.output_dir, args.remove_outliers, temp_db_file))

    # Process genes in parallel
    logger.info("Starting parallel gene processing.")
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = list(executor.map(process_gene, args_list))

    # Process the results
    num_passing_genes = sum(1 for res in results if res[2])
    num_failing_genes = sum(1 for res in results if not res[2])

    total_genes = num_passing_genes + num_failing_genes
    if total_genes == 0:
        logger.info("No genes were processed.")
    else:
        passing_percentage = (num_passing_genes / total_genes) * 100
        failing_percentage = (num_failing_genes / total_genes) * 100
        logger.info(
            f"Passing genes: {num_passing_genes} ({passing_percentage:.2f}%)\n"
            f"Failing genes: {num_failing_genes} ({failing_percentage:.2f}%)")
    logger.success("Processing complete.")

    # Clean up temporary database file
    os.remove(temp_db_file)


if __name__ == "__main__":
    main()
