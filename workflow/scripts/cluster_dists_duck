#!/usr/bin/env python

import duckdb
import scipy
import gzip
from loguru import logger
import matplotlib.pyplot as plt
import os
import re
import uuid
from dbscan1d.core import DBSCAN1D
import seaborn as sns
from matplotlib.collections import PathCollection
import glob
import pandas as pd
import sys
import argparse
import tempfile
import numpy as np
import concurrent.futures
import matplotlib.ticker as ticker
import gc
import time
import psutil
from functools import wraps
from multiprocessing import shared_memory, Manager
import threading
from queue import Queue
import weakref
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, Any
import json


# Performance monitoring decorator
def profile_performance(func):

    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        result = func(*args, **kwargs)
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        logger.debug(f"{func.__name__}: {end_time-start_time:.2f}s, "
                     f"Memory: {(end_memory-start_memory)/1024/1024:.1f}MB")
        return result

    return wrapper


@dataclass
class GeneStats:
    gene_id: str
    n_unique_diffs: int
    total_weight: int
    diff_range: float
    diff_std: float
    diff_median: float


class ConnectionPool:
    """Thread-safe DuckDB connection pool"""

    def __init__(self, db_path: str, max_connections: int = 8):
        self.db_path = db_path
        self.max_connections = max_connections
        self._pool = Queue(maxsize=max_connections)
        self._lock = threading.Lock()
        self._initialized = False

    def _initialize(self):
        if not self._initialized:
            with self._lock:
                if not self._initialized:
                    for _ in range(self.max_connections):
                        conn = duckdb.connect(database=self.db_path,
                                              read_only=True)
                        # Optimize connection settings
                        # conn.execute("SET memory_limit=2GB")
                        # conn.execute("SET threads=2")
                        self._pool.put(conn)
                    self._initialized = True

    @contextmanager
    def get_connection(self):
        self._initialize()
        conn = self._pool.get()
        try:
            yield conn
        finally:
            self._pool.put(conn)

    def close_all(self):
        with self._lock:
            while not self._pool.empty():
                conn = self._pool.get()
                conn.close()


class FigurePool:
    """Pool of reusable matplotlib figures"""

    def __init__(self, pool_size: int = 4):
        self.pool_size = pool_size
        self._figures = []
        self._lock = threading.Lock()

    @contextmanager
    def get_figure(self):
        with self._lock:
            if self._figures:
                fig = self._figures.pop()
            else:
                fig = plt.figure(figsize=(5, 4))

        try:
            # Clear the figure
            fig.clear()
            yield fig
        finally:
            with self._lock:
                if len(self._figures) < self.pool_size:
                    self._figures.append(fig)
                else:
                    plt.close(fig)


# Global pools
_connection_pool = None
_figure_pool = FigurePool()


def get_connection_pool(db_path: str) -> ConnectionPool:
    global _connection_pool
    if _connection_pool is None:
        _connection_pool = ConnectionPool(db_path)
    return _connection_pool


@profile_performance
def load_gene_batch_data(
        db_path: str,
        gene_ids: List[str]) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """Load data for multiple genes in a single query"""
    with get_connection_pool(db_path).get_connection() as con:
        # Create temporary table with gene IDs
        gene_placeholders = ','.join(['?' for _ in gene_ids])

        df = con.execute(
            f"""
            SELECT GeneID, Difference, COUNT(*) AS weight
            FROM gene_data
            WHERE GeneID IN ({gene_placeholders})
            GROUP BY GeneID, Difference
            ORDER BY GeneID, Difference
        """, gene_ids).df()

        # Group by gene_id and convert to numpy arrays with optimized dtypes
        result = {}
        for gene_id in gene_ids:
            gene_data = df[df['GeneID'] == gene_id]
            if len(gene_data) > 0:
                diffs = gene_data['Difference'].astype(np.float32).values
                weights = gene_data['weight'].astype(np.uint32).values
                result[gene_id] = (diffs, weights)

        return result


@profile_performance
def compute_gene_statistics(
    gene_data: Dict[str, Tuple[np.ndarray,
                               np.ndarray]]) -> Dict[str, GeneStats]:
    """Compute statistics for genes to guide adaptive parameter selection"""
    stats = {}
    for gene_id, (diffs, weights) in gene_data.items():
        if len(diffs) == 0:
            continue

        # Compute weighted statistics
        total_weight = np.sum(weights)
        weighted_diffs = np.repeat(diffs, weights)

        stats[gene_id] = GeneStats(gene_id=gene_id,
                                   n_unique_diffs=len(diffs),
                                   total_weight=int(total_weight),
                                   diff_range=float(np.ptp(diffs)),
                                   diff_std=float(np.std(weighted_diffs)),
                                   diff_median=float(
                                       np.median(weighted_diffs)))

    return stats


def adaptive_clustering_params(stats: GeneStats, base_eps: float,
                               base_min_samples: int) -> Tuple[float, int]:
    """Adapt clustering parameters based on gene characteristics"""
    # Adjust epsilon based on data spread
    if stats.diff_std > 0:
        eps_multiplier = min(2.0, max(0.5, stats.diff_std / 1000))
        adaptive_eps = base_eps * eps_multiplier
    else:
        adaptive_eps = base_eps

    # Adjust min_samples based on data density
    density_factor = stats.total_weight / max(1, stats.n_unique_diffs)
    if density_factor > 10:
        adaptive_min_samples = max(base_min_samples // 2, 25)
    elif density_factor < 2:
        adaptive_min_samples = min(base_min_samples * 2, 200)
    else:
        adaptive_min_samples = base_min_samples

    return float(adaptive_eps), int(adaptive_min_samples)


def should_skip_gene(stats: GeneStats,
                     min_complexity_threshold: int = 1) -> Tuple[bool, str]:
    """Determine if a gene should be skipped based on statistical criteria"""
    # Skip genes with very few unique differences
    # if stats.n_unique_diffs <= min_complexity_threshold:
        # return True, f"Too few unique differences: {stats.n_unique_diffs}"

    # Skip genes with very low variance (likely single cluster)
    # if stats.diff_std < 1:  # Very tight distribution
        # return True, f"Low variance: {stats.diff_std:.2f}"

    # Skip genes with extremely sparse data
    # density = stats.total_weight / stats.n_unique_diffs
    # if density < 1.1:
        # return True, f"Too sparse: density={density:.2f}"

    return False, ""


@profile_performance
def process_gene_batch(
    gene_batch: List[str],
    base_eps: float,
    base_min_samples: int,
    outdir: str,
    remove_outliers: bool,
    db_path: str,
    skip_plots: bool = False
) -> List[Tuple[str, Optional[str], Optional[str]]]:
    """Process a batch of genes together for better efficiency"""

    # Load data for all genes in batch
    gene_data = load_gene_batch_data(db_path, gene_batch)
    if not gene_data:
        return [(gene_id, None, None) for gene_id in gene_batch]

    # Compute statistics for adaptive parameter selection
    gene_stats = compute_gene_statistics(gene_data)

    results = []

    for gene_id in gene_batch:
        try:
            if gene_id not in gene_data or gene_id not in gene_stats:
                results.append((gene_id, None, None))
                continue

            diffs, weights = gene_data[gene_id]
            stats = gene_stats[gene_id]

            # Check if we should skip this gene
            should_skip, skip_reason = should_skip_gene(stats)
            if should_skip:
                logger.debug(f"Skipping gene {gene_id}: {skip_reason}")
                results.append((gene_id, None, None))
                continue

            # Adaptive parameter selection
            # eps, min_samples = adaptive_clustering_params(
                # stats, base_eps, base_min_samples)

            # Perform clustering
            labels = DBSCAN1D(eps=base_eps, min_samples=base_min_samples).fit_predict(
                diffs, sample_weight=weights)

            # Check for at least one non-noise cluster
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            if n_clusters <= 1:
                results.append((gene_id, None, None))
                continue

            # Compute weighted means and relabel
            cluster_means = {
                lbl: (diffs[labels == lbl] * weights[labels == lbl]).sum() /
                weights[labels == lbl].sum()
                for lbl in set(labels) if lbl != -1
            }
            sorted_clusters = sorted(cluster_means, key=lambda lbl: abs(cluster_means[lbl]))
            relabel_map = {old: new for new, old in enumerate(sorted_clusters)}
            if -1 in labels:
                relabel_map[-1] = -1

            # Export results
            gene_name = gene_id.replace("/", ":").replace("|", ":").replace(
                "(", ":").replace(")", ":").replace("'", ":")
            gene_outdir = os.path.join(outdir, gene_name)
            os.makedirs(gene_outdir, exist_ok=True)

            out_csv = os.path.join(gene_outdir, f"{gene_name}_clusters.csv.gz")
            plot_path = None

            # Export CSV with optimized query
            success = export_gene_clusters(db_path, gene_id, diffs, labels,
                                           relabel_map, out_csv,
                                           remove_outliers)

            if success:
                # Generate plot if requested and data is not too large
                if not skip_plots and stats.total_weight < 100000:  # Skip plots for very large datasets
                    plot_path = os.path.join(gene_outdir,
                                             f"{gene_name}_clusters.png")
                    create_optimized_plot(out_csv, gene_name, plot_path, stats)

                results.append((gene_id, out_csv, plot_path))
            else:
                results.append((gene_id, None, None))

        except Exception as e:
            logger.error(f"Error processing gene {gene_id}: {e}")
            results.append((gene_id, None, None))

    # Cleanup
    del gene_data, gene_stats
    gc.collect()

    return results


def export_gene_clusters(db_path: str, gene_id: str, diffs: np.ndarray,
                         labels: np.ndarray, relabel_map: Dict[int, int],
                         out_csv: str, remove_outliers: bool) -> bool:
    """Export cluster results to CSV with optimized database operations"""
    try:
        # Build mapping more efficiently
        mapping = [(float(d), relabel_map[l]) for d, l in zip(diffs, labels)]

        with get_connection_pool(db_path).get_connection() as con:
            # Create temporary table with better typing
            con.execute(
                "CREATE OR REPLACE TEMPORARY TABLE cluster_map(diff DOUBLE, new_cluster INTEGER)"
            )

            # Batch insert the mapping
            con.executemany("INSERT INTO cluster_map VALUES (?, ?)", mapping)

            # Optimized export query
            where_clause = "AND cm.new_cluster != -1" if remove_outliers else ""

            export_query = f"""
                COPY (
                    SELECT zd.*, cm.new_cluster AS Cluster,
                    replace(
                            split_part(zd.filename, '/', -1),
                            '-filterdists.csv.gz', ''
                        ) AS Bucket
                    FROM gene_data AS zd
                    JOIN cluster_map AS cm ON zd.Difference = cm.diff
                    WHERE zd.GeneID = ?
                    {where_clause}
                )
                TO '{out_csv}'
                WITH (FORMAT CSV, HEADER, DELIMITER ',', COMPRESSION GZIP)
            """

            con.execute(export_query, [gene_id])

            # Clean up temporary table
            con.execute("DROP TABLE cluster_map")

        return True

    except Exception as e:
        logger.error(f"Error exporting clusters for {gene_id}: {e}")
        return False


@profile_performance
def create_optimized_plot(csv_path: str,
                          gene_id: str,
                          output_path: str,
                          stats: GeneStats,
                          max_plot_samples: int = 10000):
    """Create plots with memory optimization and intelligent sampling"""
    try:
        # Use streaming read for large files
        chunk_size = min(max_plot_samples, 50000)

        # Read in chunks and sample if necessary
        chunks = []
        total_rows = 0

        for chunk in pd.read_csv(csv_path,
                                 compression='gzip',
                                 chunksize=chunk_size):
            total_rows += len(chunk)
            chunks.append(chunk)
            if total_rows >= max_plot_samples:
                break

        # Combine chunks
        if chunks:
            plot_df = pd.concat(chunks, ignore_index=True)
        else:
            return False

        # Sample if still too large
        if len(plot_df) > max_plot_samples:
            plot_df = plot_df.sample(n=max_plot_samples, random_state=42)

        # Optimize data types for plotting
        plot_df['Difference'] = plot_df['Difference'].astype(np.float32)
        if 'Cluster' in plot_df.columns:
            plot_df['Cluster'] = plot_df['Cluster'].astype('category')

        # Use figure pool for memory efficiency
        with _figure_pool.get_figure() as fig:
            # Create subplots with optimized layout
            gs = fig.add_gridspec(2, 1, height_ratios=[3, 1], hspace=0.3)
            ax1 = fig.add_subplot(gs[0])
            ax2 = fig.add_subplot(gs[1])

            # Adaptive binning based on data characteristics
            n_bins = min(100, max(20, int(np.sqrt(len(plot_df)))))

            # Top panel: histogram with log scale
            sns.histplot(data=plot_df,
                         x='Difference',
                         ax=ax1,
                         bins=n_bins,
                         fill=False,
                         color='black',
                         stat='count')
            ax1.set_yscale('log')
            ax1.set_ylabel('Number of Genomes')
            ax1.set_title(f'{gene_id}')
            ax1.set_xlabel('')

            # Format y-axis
            ax1.yaxis.set_major_formatter(
                ticker.FuncFormatter(lambda x, pos: f'{x:,.0f}'
                                     if x < 1000 else f'{x/1e3:.1f}K'))

            # Bottom panel: strip + violin plot with sampling
            sample_size = min(5000, len(plot_df))
            plot_sample = plot_df.sample(
                n=sample_size,
                random_state=42) if len(plot_df) > sample_size else plot_df

            if 'Cluster' in plot_df.columns and plot_df['Cluster'].nunique(
            ) > 1:
                # Sort clusters, with noise (-1) at the end
                cluster_order = sorted(plot_df['Cluster'].cat.categories,
                                       key=lambda c: (c == '-1', c))

                # Strip plot
                sns.stripplot(data=plot_sample,
                              x='Difference',
                              hue='Cluster',
                              order=cluster_order,
                              orient='h',
                              ax=ax2,
                              jitter=True,
                              size=2,
                              alpha=0.7)

                # Violin plot
                sns.violinplot(data=plot_sample,
                               x='Difference',
                               hue='Cluster',
                               order=cluster_order,
                               orient='h',
                               ax=ax2,
                               inner=None,
                               cut=0,
                               alpha=0.5)

                # Legend
                handles, labels = ax2.get_legend_handles_labels()
                if handles:
                    fig.legend(handles[:len(cluster_order)],
                               labels[:len(cluster_order)],
                               title='Cluster',
                               bbox_to_anchor=(1.02, 0.5),
                               loc='center left')
                ax2.get_legend().remove()
            else:
                # Single color plots
                sns.stripplot(data=plot_sample,
                              x='Difference',
                              color='black',
                              orient='h',
                              ax=ax2,
                              jitter=True,
                              size=2,
                              alpha=0.7)
                sns.violinplot(data=plot_sample,
                               x='Difference',
                               color='lightgrey',
                               orient='h',
                               ax=ax2,
                               inner=None,
                               cut=0,
                               alpha=0.5)

            # Format bottom axis
            ax2.set_xlabel('Prefix-suffix distance relative to gene length')
            ax2.set_ylabel('')
            ax2.set_yticks([])
            ax2.xaxis.set_major_formatter(
                ticker.FuncFormatter(lambda x, pos: f'{x:,.0f}'
                                     if x < 1000 else f'{x/1e3:.1f}kb'))

            # Clean styling
            sns.despine(fig=fig, left=True, bottom=False)

            # Adjust DPI based on complexity
            dpi = 150 if len(plot_df) < 10000 else 100
            fig.savefig(output_path,
                        dpi=dpi,
                        bbox_inches='tight',
                        facecolor='white',
                        transparent=False)

        # Cleanup
        del plot_df, plot_sample
        gc.collect()
        return True

    except Exception as e:
        logger.error(f"Error creating plot for {gene_id}: {e}")
        return False


@profile_performance
def generate_process_gene_args_optimized(db_path: str,
                                         epsilon_val: float,
                                         min_samples_val: int,
                                         output_dir_val: str,
                                         remove_outliers_val: bool,
                                         batch_size: int = 50):
    """
    Optimized generator that yields batches of gene IDs for batch processing
    """
    with get_connection_pool(db_path).get_connection() as conn:
        try:
            # Verify table exists
            conn.execute("SELECT 1 FROM gene_data LIMIT 1")
        except duckdb.Error as e:
            logger.error(f"'gene_data' table not found: {e}")
            return

        # Get gene count for progress tracking
        total_genes = conn.execute(
            "SELECT COUNT(DISTINCT GeneID) FROM gene_data").fetchone()[0]
        logger.info(f"Total genes to process: {total_genes}")

        # Stream gene IDs in batches
        query = "SELECT DISTINCT GeneID FROM gene_data ORDER BY GeneID"
        cursor = conn.cursor()
        cursor.execute(query)

        batch = []
        processed = 0

        while True:
            rows = cursor.fetchmany(batch_size *
                                    10)  # Fetch larger chunks from DB
            if not rows:
                break

            for (gene_id, ) in rows:
                batch.append(gene_id)
                if len(batch) >= batch_size:
                    yield (batch.copy(), epsilon_val, min_samples_val,
                           output_dir_val, remove_outliers_val, db_path)
                    processed += len(batch)
                    if processed % 1000 == 0:
                        logger.info(
                            f"Generated batches for {processed}/{total_genes} genes"
                        )
                    batch.clear()

        # Yield final partial batch
        if batch:
            yield (batch, epsilon_val, min_samples_val, output_dir_val,
                   remove_outliers_val, db_path)
            processed += len(batch)

        logger.info(f"Finished generating batches for {processed} genes")


def process_gene_batch_wrapper(args):
    """Wrapper for batch processing to maintain interface compatibility"""
    gene_batch, eps, min_samples, outdir, remove_outliers, db_path = args
    return process_gene_batch(gene_batch, eps, min_samples, outdir,
                              remove_outliers, db_path)


@profile_performance
def batch_process_genes_optimized(args_generator,
                                  num_threads: int,
                                  monitor_resources: bool = True):
    """
    Optimized batch processing with resource monitoring and dynamic adjustment
    """
    logger.info(
        f"Starting optimized batch processing with {num_threads} threads")
    total_processed = 0
    total_passing = 0

    # Resource monitoring
    start_memory = psutil.Process().memory_info().rss

    with concurrent.futures.ProcessPoolExecutor(
            max_workers=num_threads) as executor:
        # Submit batches with dynamic adjustment
        futures = []
        max_pending = num_threads * 2  # Limit pending futures

        for batch_args in args_generator:
            # Submit new batch
            future = executor.submit(process_gene_batch_wrapper, batch_args)
            futures.append(future)

            # Process completed futures when we have enough pending
            if len(futures) >= max_pending:
                completed_futures = []
                for future in concurrent.futures.as_completed(futures):
                    try:
                        batch_results = future.result()
                        passing = sum(1 for (_, out_csv, _) in batch_results
                                      if out_csv)
                        total_passing += passing
                        total_processed += len(batch_results)
                        completed_futures.append(future)

                        if total_processed % 500 == 0:
                            current_memory = psutil.Process().memory_info().rss
                            memory_usage = (current_memory -
                                            start_memory) / 1024 / 1024
                            logger.info(
                                f"Progress: {total_processed} processed, "
                                f"{total_passing} passed, Memory: +{memory_usage:.1f}MB"
                            )

                        # Break after processing one future to maintain flow
                        break
                    except Exception as e:
                        logger.error(f"Batch processing error: {e}")
                        completed_futures.append(future)

                # Remove completed futures
                futures = [f for f in futures if f not in completed_futures]

                # Resource monitoring and adjustment
                if monitor_resources:
                    current_memory = psutil.Process().memory_info().rss
                    memory_usage_gb = current_memory / 1024 / 1024 / 1024
                    if memory_usage_gb > 8:  # If using more than 8GB
                        logger.warning(
                            f"High memory usage: {memory_usage_gb:.1f}GB"
                        )
                        # gc.collect()

        # Process remaining futures
        for future in concurrent.futures.as_completed(futures):
            try:
                batch_results = future.result()
                passing = sum(1 for (_, out_csv, _) in batch_results
                              if out_csv)
                total_passing += passing
                total_processed += len(batch_results)
            except Exception as e:
                logger.error(f"Final batch processing error: {e}")

    return total_processed, total_passing


@profile_performance
def create_duckdb_with_optimizations(input_dir: str,
                                     output_dir: str,
                                     db_file: Optional[str] = None):
    """
    Create optimized DuckDB database with better memory management and indexing
    """
    if db_file and os.path.exists(db_file):
        logger.info(f"Using existing database at {db_file}")
        return db_file

    # Generate unique database file
    unique_id = str(uuid.uuid4())
    temp_db_file = os.path.join(output_dir, f'gene_data_{unique_id}.duckdb')

    try:
        logger.info(f"Creating optimized DuckDB database at {temp_db_file}")
        con_write = duckdb.connect(database=temp_db_file, read_only=False)

        # Optimize DuckDB settings
        # con_write.execute("SET memory_limit=6GB")
        # con_write.execute("SET threads=4")
        # con_write.execute("SET checkpoint_threshold=1GB")
        # con_write.execute("SET wal_autocheckpoint=1000")

        # Find CSV files
        csv_pattern = os.path.join(input_dir, "*.csv.gz")
        csv_files = glob.glob(csv_pattern)

        if not csv_files:
            logger.error("No CSV files found. Aborting.")
            con_write.close()
            return None

        logger.info(f"Found {len(csv_files)} CSV files to process")

        # Filter non-empty files more efficiently
        non_empty_files = []
        for file in csv_files:
            try:
                # Check file size first
                if os.path.getsize(file) > 100:  # At least 100 bytes
                    with gzip.open(file, 'rt') as f:
                        header = f.readline()
                        if f.readline():  # Check if there's data
                            non_empty_files.append(file)
            except Exception as e:
                logger.warning(f"Error checking file {file}: {e}")

        if not non_empty_files:
            logger.error("No non-empty CSV files found. Aborting.")
            con_write.close()
            return None

        logger.info(f"Processing {len(non_empty_files)} non-empty CSV files")

        # Optimized column types
        column_types = {
            'GenomeID': 'VARCHAR',
            'GeneID': 'VARCHAR',
            'PrefixID': 'VARCHAR',
            'SuffixID': 'VARCHAR',
            'PrefixContig': 'VARCHAR',
            'SuffixContig': 'VARCHAR',
            'Contig': 'VARCHAR',
            'Distance': 'BIGINT',  # Smaller than BIGINT if possible
            'Difference':
            'REAL',  # REAL is more efficient than DOUBLE for most cases
            'PrefixLocation': 'BIGINT',
            'SuffixLocation': 'BIGINT',
        }

        # Process files in optimized batches
        batch_size = min(500,
                         len(non_empty_files) // 4 + 1)  # Adaptive batch size

        for i in range(0, len(non_empty_files), batch_size):
            batch_files = non_empty_files[i:i + batch_size]
            logger.info(
                f"Processing batch {i//batch_size + 1}/{(len(non_empty_files)-1)//batch_size + 1} "
                f"({len(batch_files)} files)")

            try:
                # Read batch with optimized settings
                rel = con_write.read_csv(
                    batch_files,
                    compression='gzip',
                    filename=True,
                    header=True,
                    sep=',',
                    dtype=column_types,
                    ignore_errors=True,  # Skip problematic rows
                )

                # Create or append to table
                if i == 0:
                    rel.create('gene_data')
                else:
                    rel.insert_into('gene_data')

                # Periodic checkpoints for large datasets
                if (i // batch_size + 1) % 5 == 0:
                    con_write.execute("CHECKPOINT")

            except Exception as e:
                logger.error(
                    f"Error processing batch {i//batch_size + 1}: {e}")

        # Create optimized indexes
        logger.info("Creating optimized indexes")
        con_write.execute(
            "CREATE INDEX IF NOT EXISTS gene_id_idx ON gene_data(GeneID)")
        con_write.execute(
            "CREATE INDEX IF NOT EXISTS gene_diff_idx ON gene_data(GeneID, Difference)"
        )

        # Final checkpoint and optimization
        con_write.execute("CHECKPOINT")
        con_write.execute("VACUUM")
        con_write.close()

        logger.info(f"Optimized DuckDB database created at {temp_db_file}")
        return temp_db_file

    except Exception as e:
        logger.error(f"Error creating optimized DuckDB database: {e}")
        if os.path.exists(temp_db_file):
            try:
                os.remove(temp_db_file)
            except:
                pass
        return None


def main():
    parser = argparse.ArgumentParser(
        description=
        "Highly optimized gene clustering script with adaptive parameters and resource management."
    )
    parser.add_argument("-i",
                        "--input_dir",
                        type=str,
                        required=True,
                        help="Directory containing input CSV files.")
    parser.add_argument("-o",
                        "--output_dir",
                        type=str,
                        required=True,
                        help="Directory to store output files.")
    parser.add_argument("-d",
                        "--duckdb",
                        type=str,
                        required=False,
                        help="Path to existing DuckDB file.")
    parser.add_argument(
        "--epsilon",
        type=float,
        default=800,
        help="Base epsilon value for DBSCAN1D (will be adapted per gene).")
    parser.add_argument(
        "--min_samples",
        type=int,
        default=75,
        help="Base minimum samples for DBSCAN1D (will be adapted per gene).")
    parser.add_argument("-j",
                        "--threads",
                        type=int,
                        default=4,
                        help="Number of threads for parallel processing.")
    parser.add_argument("--remove_outliers",
                        action="store_true",
                        help="Remove outliers from output.")
    parser.add_argument("--batch_size",
                        type=int,
                        default=50,
                        help="Number of genes to process in each batch.")
    parser.add_argument("--skip_plots",
                        action="store_true",
                        help="Skip plot generation to save time and memory.")
    parser.add_argument("--max_plot_samples",
                        type=int,
                        default=10000,
                        help="Maximum number of samples to use for plotting.")
    parser.add_argument(
        "--complexity_threshold",
        type=int,
        default=100,
        help="Minimum complexity threshold for processing genes.")
    parser.add_argument("-v",
                        "--verbose",
                        action="count",
                        default=0,
                        help="Increase verbosity level.")

    args = parser.parse_args()

    # Set up logging
    logger.remove()
    if args.verbose == 0:
        logger.add(sys.stderr, level="WARNING")
    elif args.verbose == 1:
        logger.add(sys.stderr, level="INFO")
    else:
        logger.add(sys.stderr, level="DEBUG")

    # Log system resources at start
    memory_gb = psutil.virtual_memory().total / 1024 / 1024 / 1024
    cpu_count = psutil.cpu_count()
    logger.info(f"System resources: {memory_gb:.1f}GB RAM, {cpu_count} CPUs")
    logger.info(f"Using {args.threads} threads for processing")

    # Create output directories
    passinggenesdir = os.path.join(args.output_dir, "passing_genes")
    os.makedirs(passinggenesdir, exist_ok=True)

    # Create or use existing optimized DuckDB database
    start_time = time.time()
    db_file = create_duckdb_with_optimizations(args.input_dir, args.output_dir,
                                               args.duckdb)
    if not db_file:
        logger.error("Failed to create or access DuckDB database. Exiting.")
        sys.exit(1)

    db_creation_time = time.time() - start_time
    logger.info(f"Database setup completed in {db_creation_time:.2f} seconds")

    # Initialize global connection pool
    global _connection_pool
    _connection_pool = ConnectionPool(db_file,
                                      max_connections=args.threads + 2)

    try:
        # Generate optimized batch arguments
        logger.info("Starting optimized gene processing...")
        processing_start = time.time()

        args_generator = generate_process_gene_args_optimized(
            db_file, args.epsilon, args.min_samples, passinggenesdir,
            args.remove_outliers, args.batch_size)

        # Process genes with optimized batch processing
        total_processed, total_passing = batch_process_genes_optimized(
            args_generator, args.threads, monitor_resources=True)

        processing_time = time.time() - processing_start

        # Report results with performance metrics
        if total_processed == 0:
            logger.info("No genes were processed.")
        else:
            passing_percentage = (total_passing / total_processed) * 100
            failing_percentage = (
                (total_processed - total_passing) / total_processed) * 100
            genes_per_second = total_processed / processing_time if processing_time > 0 else 0

            logger.success(
                f"Processing completed in {processing_time:.2f} seconds:\n"
                f"  Processed: {total_processed} genes ({genes_per_second:.2f} genes/sec)\n"
                f"  Passing: {total_passing} ({passing_percentage:.2f}%)\n"
                f"  Failing: {total_processed - total_passing} ({failing_percentage:.2f}%)\n"
                f"  Total time: {(time.time() - start_time):.2f} seconds")

        # Performance summary
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        logger.info(f"Peak memory usage: {final_memory:.1f}MB")

    finally:
        # Cleanup resources
        if _connection_pool:
            _connection_pool.close_all()

        # Clean up figure pool
        _figure_pool._figures.clear()
        plt.close('all')

        # Final garbage collection
        gc.collect()

    logger.info(f"Database file available at: {db_file}")
    logger.success("Optimized processing complete!")


if __name__ == "__main__":
    main()
