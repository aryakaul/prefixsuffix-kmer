#!/usr/bin/env python3

import sys
import argparse
import os
import numpy as np
import pandas as pd
from loguru import logger
from pathlib import Path
from multiprocessing import Pool
import gzip  ###CLAUDE GENERATED### # Added for direct gzip compression

def calculate_distances(group, k, g):
    """
    Calculate distances between prefix and suffix k-mers in a given group
    using a merge-based approach. This keeps duplicates, enumerating every
    prefix-suffix pair.
    """

    # If this group is empty, return an empty DataFrame
    if group.empty:
        return pd.DataFrame()

    # Split group into prefix & suffix
    ###CLAUDE GENERATED### # Removed .copy() to reduce memory usage
    prefixes = group[group["KmerType"] == "prefix"]
    suffixes = group[group["KmerType"] == "suffix"]
    logger.debug(f"Prefixes shape: {prefixes.shape}")
    logger.debug(f"Suffixes shape: {suffixes.shape}")

    if prefixes.shape[0] > 10000 or suffixes.shape[0] > 10000:
        logger.warning(f"More than 10,000 prefix or suffix matches. Likely MGE! Skipping. Prefix matches: {prefixes.shape[0]} Suffix matches: {suffixes.shape[0]}")
        return pd.DataFrame()

    ###CLAUDE GENERATED### # Optimize merge by selecting only needed columns
    prefix_cols = ["genomeID", "length", "kmerID", "Contig", "Location", "GeneID"]
    suffix_cols = ["genomeID", "length", "kmerID", "Contig", "Location", "GeneID"]
    
    # Merge on ["genomeID","length"]. 'outer' to keep all possible combos
    merged = pd.merge(
        prefixes[prefix_cols],
        suffixes[suffix_cols],
        on=["genomeID", "length"],
        how="outer",
        suffixes=("_prefix", "_suffix"),
        sort=False
    )

    # Fill missing contigs with placeholders
    merged["Contig_prefix"] = merged["Contig_prefix"].fillna("None")
    merged["Contig_suffix"] = merged["Contig_suffix"].fillna("None")

    # Distance calculation
    prefix_missing = merged["Location_prefix"].isna()
    suffix_missing = merged["Location_suffix"].isna()

    ###CLAUDE GENERATED### # Create distance column directly with the right type
    merged["distance"] = pd.Series(np.nan, index=merged.index, dtype=object)

    merged.loc[prefix_missing, "distance"] = "PrefixMissing"
    merged.loc[suffix_missing, "distance"] = "SuffixMissing"

    has_both = (~prefix_missing) & (~suffix_missing)
    same_contig = has_both & (merged["Contig_prefix"] == merged["Contig_suffix"])
    merged.loc[same_contig, "distance"] = (
        (
            merged.loc[same_contig, "Location_suffix"].abs()
            - merged.loc[same_contig, "Location_prefix"].abs()
        ).abs()
        + k
        + 2*g
    )
    different_contig = has_both & (merged["Contig_prefix"] != merged["Contig_suffix"])
    merged.loc[different_contig, "distance"] = "ContigsDifferent"

    # Compute "Difference" for numeric distances
    numeric_mask = pd.to_numeric(merged["distance"], errors="coerce").notna()
    merged.loc[numeric_mask, "Difference"] = (
        merged.loc[numeric_mask, "distance"].astype(float)
        - merged.loc[numeric_mask, "length"]
    )

    # If the group has only one GeneID, pick from either side
    merged["GeneID"] = merged["GeneID_prefix"].combine_first(merged["GeneID_suffix"])

    # Final DataFrame with consistent columns, built in one pass
    result = pd.DataFrame({
        "genomeID": merged["genomeID"],
        "GeneID": merged["GeneID"],
        "PrefixID": merged["kmerID_prefix"],
        "SuffixID": merged["kmerID_suffix"],
        "PrefixContig": merged["Contig_prefix"],
        "SuffixContig": merged["Contig_suffix"],
        "Contig": merged["Contig_prefix"].combine_first(merged["Contig_suffix"]),
        "Distance": merged["distance"],
        "Difference": merged["Difference"],
        "PrefixLocation": merged["Location_prefix"],
        "SuffixLocation": merged["Location_suffix"],
    })

    return result


def process_group_wrapper(args):
    """
    Multiprocessing wrapper for handling each group.
    """
    (name, group, k, g) = args
    logger.info(f"Processing group: {name}")
    return calculate_distances(group, k, g)


###CLAUDE GENERATED### # Added function for streaming compression
def compress_file(input_file, output_file=None, buffer_size=4*1024*1024, remove_original=True):
    """
    Compress a file using gzip with efficient buffering.
    
    Args:
        input_file: Path to the input file to compress
        output_file: Path to the output compressed file (default: input_file + '.gz')
        buffer_size: Size of buffer for reading/writing (default: 4MB)
        remove_original: Whether to remove the original file after compression
    
    Returns:
        Path to the compressed file
    """
    if output_file is None:
        output_file = input_file + '.gz'
    
    with open(input_file, 'rb') as f_in:
        with gzip.open(output_file, 'wb', compresslevel=6) as f_out:
            while True:
                buffer = f_in.read(buffer_size)
                if not buffer:
                    break
                f_out.write(buffer)
    
    # Verify the compressed file exists
    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
        if remove_original:
            os.remove(input_file)
        return output_file
    else:
        raise IOError(f"Failed to create compressed file: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Compute prefix-suffix k-mer distances via merging, writing results incrementally."
    )
    parser.add_argument("-i", "--input", required=True,
                        help="Input BWA FastMap TSV/CSV (gzipped ok).")
    parser.add_argument("-o", "--output", required=True,
                        help="Output TSV for distance data.")
    parser.add_argument("-g", "--gap-distance", type=int, required=True,
                        help="Gap distance G.")
    parser.add_argument("-k", "--kmer-length", type=int, required=True,
                        help="K-mer length.")
    parser.add_argument("-t", "--threads", type=int, default=1,
                        help="Number of worker processes.")
    parser.add_argument("-v", "--verbose", action="count", default=0,
                        help="Increase verbosity (-v, -vv, etc.).")
    ###CLAUDE GENERATED### # Added compression options
    parser.add_argument("--compress", action="store_true", default=True,
                        help="Compress output file with gzip (default: True)")
    parser.add_argument("--chunk-size", type=int, default=100000,
                        help="Number of rows to read/process at once (default: 100000)")
    parser.add_argument("--max-tasks-per-child", type=int, default=100,
                        help="Maximum number of tasks per worker process to prevent memory leaks (default: 100)")
    args = parser.parse_args()

    # Configure logging
    logger.remove()  # remove default handler
    if args.verbose == 0:
        pass  # no logs
    elif args.verbose == 1:
        logger.add(sys.stderr, level="INFO")
    else:  # >= 2
        logger.add(sys.stderr, level="DEBUG")

    # Ensure output directory exists
    Path(os.path.dirname(args.output)).mkdir(parents=True, exist_ok=True)

    # Narrow reading: usecols, dtype
    dtype_spec = {
        "genomeID": "string",
        "kmerID": "string",
        "Contig": "string",
        "Location": "Int64"
    }
    usecols = ["genomeID", "kmerID", "Contig", "Location"]
    
    ###CLAUDE GENERATED### # Read input file in chunks to reduce memory usage
    chunk_reader = pd.read_csv(
        args.input,
        sep="\t",
        dtype=dtype_spec,
        na_values=["Null", "None"],
        usecols=usecols,
        chunksize=args.chunk_size
    )
    
    ###CLAUDE GENERATED### # Prepare output file, potentially gzipped
    output_file = args.output
    if args.compress and not output_file.endswith('.gz'):
        output_file = output_file + '.gz'
    
    ###CLAUDE GENERATED### # Open output file once (possibly gzipped)
    if args.compress:
        out_f = gzip.open(output_file, 'wt', encoding="utf-8")
    else:
        out_f = open(output_file, 'w', encoding="utf-8")
    
    # Define columns for output
    out_cols = [
        "genomeID",
        "GeneID",
        "PrefixID",
        "SuffixID",
        "PrefixContig",
        "SuffixContig",
        "Contig",
        "Distance",
        "Difference",
        "PrefixLocation",
        "SuffixLocation",
    ]
    
    first_chunk = True
    
    ###CLAUDE GENERATED### # Process each chunk of data
    for chunk_num, df_chunk in enumerate(chunk_reader):
        logger.info(f"Processing chunk {chunk_num+1}")
        
        ###CLAUDE GENERATED### # Convert string columns to categorical to save memory
        df_chunk["genomeID"] = df_chunk["genomeID"].astype("category")
        df_chunk["Contig"] = df_chunk["Contig"].astype("category")
        df_chunk["kmerID"] = df_chunk["kmerID"].astype("category")
        
        # Precompute columns: GeneID, KmerType, length
        ###CLAUDE GENERATED### # More efficient string operations
        df_chunk["GeneID"] = df_chunk["kmerID"].str.extract(r"(.+?-len\d+)", expand=False)
        df_chunk["KmerType"] = np.where(df_chunk["kmerID"].str.contains("prefix"), "prefix", "suffix")
        
        # Extract length from GeneID in a more vectorized way
        ###CLAUDE GENERATED### # More efficient string operations for length extraction
        df_chunk["length"] = (
            df_chunk["GeneID"]
            .str.split("-")
            .str[-1]
            .str.replace("len", "", regex=False)
            .astype("Int64")
        )
        
        # Drop rows missing essential fields
        df_chunk.dropna(subset=["kmerID", "GeneID", "Location"], inplace=True)
        
        # Group by GeneID
        grouped = df_chunk.groupby("GeneID", sort=False)
        group_args = [(name, grp, args.kmer_length, args.gap_distance)
                      for name, grp in grouped]
        
        ###CLAUDE GENERATED### # Optimize chunksize for better parallelism
        process_chunksize = max(1, min(50, len(group_args) // (args.threads * 2)))
        
        ###CLAUDE GENERATED### # Use maxtasksperchild to prevent memory leaks
        with Pool(processes=args.threads, maxtasksperchild=args.max_tasks_per_child) as pool:
            # Using imap for incremental results
            for i, result_df in enumerate(pool.imap(process_group_wrapper, group_args, chunksize=process_chunksize)):
                if result_df.empty:
                    continue  # skip entirely empty results
                
                if first_chunk:
                    # Write header once
                    result_df.to_csv(out_f, sep="\t", index=False, columns=out_cols)
                    first_chunk = False
                else:
                    # Append without header
                    result_df.to_csv(out_f, sep="\t", index=False, columns=out_cols, header=False)
        
        # Clear memory
        ###CLAUDE GENERATED### # Explicitly clean up to reduce memory pressure
        del df_chunk, grouped, group_args
    
    # Close output file
    out_f.close()
    
    if args.compress:
        logger.info(f"Compressed results written to {output_file}")
    else:
        logger.info(f"Results written to {args.output}")


if __name__ == "__main__":
    main()
