#!/usr/bin/env python

import duckdb
import scipy
import gzip
from loguru import logger
import matplotlib.pyplot as plt
import os
import re
import uuid
from dbscan1d.core import DBSCAN1D
import seaborn as sns
from matplotlib.collections import PathCollection
import glob
import pandas as pd
import sys
import argparse
import tempfile
import numpy as np
import concurrent.futures
import matplotlib.ticker as ticker
import gc


def load_gene_differences(db_file, gene_id):
    con = duckdb.connect(database=db_file, read_only=True)

    df = con.execute(
        """
        SELECT Difference, COUNT(*) AS weight
        FROM gene_data
        WHERE GeneID = ?
        GROUP BY Difference
        ORDER BY Difference
    """, [gene_id]).df()
    con.close()
    diffs = df['Difference'].to_numpy(dtype=np.int32)
    weights = df['weight'].to_numpy(dtype=np.int32)
    return diffs, weights


def process_gene(args):
    gene_id, eps, min_samples, outdir, remove_outliers, db_file = args

    # 1) Fetch unique distances + counts
    diffs, weights = load_gene_differences(db_file, gene_id)
    if len(diffs) < min_samples * 2:
        return gene_id, None, None

    # 2) Cluster on (diffs, weights)
    labels = DBSCAN1D(eps=eps, min_samples=min_samples) \
               .fit_predict(diffs, sample_weight=weights)

    # 3) Check for at least one non-noise cluster
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    if n_clusters < 1:
        return gene_id, None, None

    # 4) Compute weighted means -> relabel mapping
    cluster_means = {
        lbl: (diffs[labels == lbl] * weights[labels == lbl]).sum() /
        weights[labels == lbl].sum()
        for lbl in set(labels) if lbl != -1
    }
    sorted_clusters = sorted(cluster_means, key=lambda lbl: cluster_means[lbl])
    relabel_map = {old: new for new, old in enumerate(sorted_clusters)}
    if -1 in labels:
        relabel_map[-1] = -1

    # 5) Build mapping list and load into DuckDB
    mapping = [(float(d), relabel_map[l]) for d, l in zip(diffs, labels)]
    con = duckdb.connect(database=db_file, read_only=True)
    con.execute(
        "CREATE TEMPORARY TABLE cluster_map(diff DOUBLE, new_cluster INT)")
    con.executemany("INSERT INTO cluster_map VALUES (?, ?)", mapping)

    # Ensure output directory exists
    gene_name = gene_id.replace("/", ":").replace("|", ":").replace(
        "(", ":").replace(")", ":").replace("'", ":")
    gene_outdir = os.path.join(outdir, gene_name)
    os.makedirs(gene_outdir, exist_ok=True)

    # 6) Export to gzipped CSV
    out_csv = os.path.join(gene_outdir, f"{gene_name}_clusters.csv.gz")
    gzip_option = "GZIP"
    where_clause = ("AND cm.new_cluster != -1" if remove_outliers else "")
    con.execute(
        f"""
        COPY (
            SELECT zd.*, cm.new_cluster AS Cluster,
            replace(
                    split_part(zd.filename, '/', -1),
                    '-filterdists.csv.gz', ''
                ) AS Bucket
            FROM gene_data AS zd
            JOIN cluster_map AS cm
              ON zd.Difference = cm.diff
            WHERE zd.GeneID = ?
            {where_clause}
        )
        TO '{out_csv}'
        WITH (FORMAT CSV, HEADER, DELIMITER ',', COMPRESSION {gzip_option})
    """, [gene_id])
    con.close()

    # 7) Load the gzipped CSV and generate the plot
    plot_path = os.path.join(gene_outdir, f"{gene_name}_clusters.png")
    df = pd.read_csv(out_csv, compression='gzip')
    create_plot(df, gene_name, plot_path)

    # 8) Cleanup and return
    del df
    gc.collect()
    return gene_id, out_csv, plot_path


def create_plot(big_df, gene_id, output, max_samples=5000):
    """
    Create and save a two-panel diagnostic plot for prefix-suffix distances.
    - Top: log-scaled histogram of distances.
    - Bottom: overlaid strip + violin plots of distances by cluster.
    Params:
        big_df: DataFrame with at least 'Difference' and optionally 'Cluster'.
        gene_id: Identifier for labeling the plot.
        output: File path (e.g., .png) to save the figure.
        max_samples: Maximum points for strip/violin sampling to limit memory.
    """
    try:
        # Copy only needed columns and downcast for memory
        cols = ['Difference'
                ] + (['Cluster'] if 'Cluster' in big_df.columns else [])
        plot_df = big_df[cols].copy()
        plot_df['Difference'] = plot_df['Difference'].astype('float32')
        if 'Cluster' in plot_df.columns:
            plot_df['Cluster'] = plot_df['Cluster'].astype('category')

        # Prepare figure and axes
        fig, axs = plt.subplots(2,
                                1,
                                figsize=(5, 4),
                                sharex=True,
                                gridspec_kw={'height_ratios': [3, 1]})

        # Adaptive histogram binning
        n = len(plot_df)
        bins = min(100, max(20, n // 100))
        sns.histplot(data=plot_df,
                     x='Difference',
                     ax=axs[0],
                     bins=bins,
                     fill=False,
                     color='black')
        axs[0].set_yscale('log')
        axs[0].set_ylabel('Number of Genomes')
        axs[0].set_title(f'{gene_id}')
        axs[0].set_xlabel('')
        axs[0].yaxis.set_major_formatter(
            ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x) if x == 0
                                 else '{:,.3g}'.format(x / 1e3) + 'K'))

        # Sample for strip/violin
        sample_size = min(max_samples, n)
        plot_sample = (plot_df.sample(n=sample_size, random_state=42)
                       if n > sample_size else plot_df)

        # Plot strip + violin
        ax = axs[1]
        if 'Cluster' in plot_df.columns:
            # Sort categories so noise at end
            order = sorted(plot_df['Cluster'].cat.categories,
                           key=lambda c: ('Z' if c == '-1' else c))
            sns.stripplot(x='Difference',
                          data=plot_sample,
                          hue='Cluster',
                          order=order,
                          orient='h',
                          ax=ax,
                          jitter=True,
                          size=3,
                          alpha=0.8)
            sns.violinplot(x='Difference',
                           data=plot_sample,
                           order=order,
                           inner=None,
                           cut=0,
                           orient='h',
                           ax=ax,
                           split=False,
                           alpha=0.6)
            # Legend
            handles, labels = ax.get_legend_handles_labels()
            fig.legend(handles,
                       labels,
                       title='Cluster',
                       bbox_to_anchor=(1.02, 0.5),
                       loc='center left')
            ax.get_legend().remove()
        else:
            sns.stripplot(x='Difference',
                          data=plot_sample,
                          color='black',
                          orient='h',
                          ax=ax,
                          jitter=True,
                          size=3,
                          alpha=0.8)
            sns.violinplot(x='Difference',
                           data=plot_sample,
                           inner=None,
                           cut=0,
                           orient='h',
                           ax=ax,
                           color='lightgrey',
                           alpha=0.6)

        # Formatting second axis
        ax.set_xlabel('Prefix-suffix distance relative to gene length')
        ax.set_ylabel('')
        ax.set_yticks([])
        ax.xaxis.set_major_formatter(
            ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x) if x == 0
                                 else '{:,.3g}'.format(x / 1e3) + 'kb'))

        sns.despine(fig=fig, left=True, bottom=False)
        plt.tight_layout()
        fig.savefig(output, dpi=300, transparent=True)
        plt.close(fig)

    except Exception as e:
        import logging
        logging.getLogger(__name__).error(
            f"Error creating plot for {gene_id}: {e}")
        plt.close('all')

    finally:
        # Clean up locals and collect
        for var in ['plot_df', 'plot_sample', 'order']:
            if var in locals():
                del locals()[var]
        gc.collect()


def generate_process_gene_args(db_path, epsilon_val, min_samples_val,
                               output_dir_val, remove_outliers_val):
    """
    Generator function to yield arguments for process_gene.
    Streams GeneIDs from the database in batches to minimize memory.
    """
    conn = None
    try:
        logger.debug(f"Generator connecting to DB: {db_path}")
        conn = duckdb.connect(database=db_path, read_only=True)

        # make sure gene_data exists
        try:
            conn.execute("SELECT 1 FROM gene_data LIMIT 1")
        except duckdb.Error as e:
            logger.error(f"Generator: 'gene_data' table not found: {e}")
            return

        query = "SELECT DISTINCT GeneID FROM gene_data ORDER BY GeneID"
        cursor = conn.cursor()
        cursor.execute(query)

        batch_size = 1000
        gene_count = 0

        while True:
            rows = cursor.fetchmany(batch_size)
            if not rows:
                break
            for (gene_id, ) in rows:
                gene_count += 1
                yield (gene_id, epsilon_val, min_samples_val, output_dir_val,
                       remove_outliers_val, db_path)

        logger.info(f"Generator: Finished yielding {gene_count} GeneIDs.")
    except Exception as e:
        logger.error(f"Exception in generate_process_gene_args: {e}")
    finally:
        if conn:
            conn.close()


def process_batch(batch_args, num_threads):
    """
    Process a batch of genes in parallel and return a list of
    (gene_id, out_csv_path, plot_path).
    """
    chunksize = max(1, min(len(batch_args) // (num_threads * 4), 10))
    with concurrent.futures.ProcessPoolExecutor(
            max_workers=num_threads) as executor:
        return list(executor.map(process_gene, batch_args,
                                 chunksize=chunksize))


def batch_process_genes(args_generator, num_threads, batch_size=10_000):
    """
    Process genes in batches to limit memory usage.
    Returns: (total_processed, total_passing)
    """
    logger.info(f"Starting batch processing with {num_threads} threads")
    total_processed = 0
    total_passing = 0
    batch_args = []

    for args in args_generator:
        batch_args.append(args)
        if len(batch_args) >= batch_size:
            logger.info(f"Processing batch of {len(batch_args)} genes")
            batch_results = process_batch(batch_args, num_threads)

            # Now that process_gene returns (gene_id, out_csv, plot_path),
            # we count a “pass” whenever out_csv is non-None:
            passing = sum(1 for (_gid, out_csv, _plot) in batch_results
                          if out_csv)
            total_passing += passing
            total_processed += len(batch_args)

            logger.info(
                f"Batch completed. Genes passed: {passing}/{len(batch_args)}")
            logger.info(
                f"Total progress: {total_processed} processed, {total_passing} passed"
            )

            batch_args.clear()
            gc.collect()

    # final partial batch
    if batch_args:
        logger.info(f"Processing final batch of {len(batch_args)} genes")
        batch_results = process_batch(batch_args, num_threads)
        passing = sum(1 for (_gid, out_csv, _plot) in batch_results if out_csv)
        total_passing += passing
        total_processed += len(batch_args)
        logger.info(f"Final batch: {passing}/{len(batch_args)} passed")

    return total_processed, total_passing


def create_duckdb_with_index(input_dir, output_dir, db_file=None):
    """
    Create a DuckDB database with an index on GeneID for faster queries.
    """
    if db_file and os.path.exists(db_file):
        logger.info(f"Using existing database at {db_file}")
        return db_file

    # Generate a unique database file
    unique_id = str(uuid.uuid4())
    temp_db_file = os.path.join(output_dir, f'gene_data_{unique_id}.duckdb')

    try:
        logger.info(f"Creating new DuckDB database at {temp_db_file}")
        con_write = duckdb.connect(database=temp_db_file, read_only=False)

        # Find all CSV files
        csv_pattern = os.path.join(input_dir, "*.csv.gz")
        csv_files = glob.glob(csv_pattern)

        if not csv_files:
            logger.error("No CSV files found. Aborting.")
            con_write.close()
            return None

        logger.info(f"Found {len(csv_files)} CSV files to process")

        # Check for non-empty files
        non_empty_files = []
        for file in csv_files:
            try:
                with gzip.open(file, 'rt') as f:
                    header = f.readline()
                    first_line = f.readline()
                    if first_line:
                        non_empty_files.append(file)
            except Exception as e:
                logger.warning(f"Error checking file {file}: {e}")

        if not non_empty_files:
            logger.error("No non-empty CSV files found. Aborting.")
            con_write.close()
            return None

        logger.info(f"Processing {len(non_empty_files)} non-empty CSV files")

        # Define column types with minimal memory usage where possible
        column_types = {
            'GenomeID': 'VARCHAR',
            'GeneID': 'VARCHAR',
            'PrefixID': 'VARCHAR',
            'SuffixID': 'VARCHAR',
            'PrefixContig': 'VARCHAR',
            'SuffixContig': 'VARCHAR',
            'Contig': 'VARCHAR',
            'Distance': 'BIGINT',
            'Difference': 'DOUBLE',
            'PrefixLocation': 'BIGINT',
            'SuffixLocation': 'BIGINT',
        }

        # Process files in chunks to save memory
        batch_size = 1000  # Adjust based on file sizes and available memory

        for i in range(0, len(non_empty_files), batch_size):
            batch_files = non_empty_files[i:i + batch_size]
            logger.info(
                f"Processing batch of {len(batch_files)} files ({i+1}-{i+len(batch_files)} of {len(non_empty_files)})"
            )

            try:
                # Read this batch of CSV files
                rel = con_write.read_csv(
                    batch_files,
                    compression='gzip',
                    filename=True,
                    header=True,
                    sep=',',
                    dtype=column_types,
                )

                # For the first batch, create the table; for subsequent batches, append
                if i == 0:
                    rel.create('gene_data')
                else:
                    rel.insert_into('gene_data')

                # Force a checkpoint to write data to disk
                con_write.execute("CHECKPOINT")

            except Exception as e:
                logger.error(
                    f"Error processing batch {i//batch_size + 1}: {e}")

        # Create an index on GeneID for faster queries
        logger.info("Creating index on GeneID column")
        con_write.execute(
            "CREATE INDEX IF NOT EXISTS gene_id_idx ON gene_data(GeneID)")

        # Final checkpoint
        con_write.execute("CHECKPOINT")
        con_write.close()

        logger.info(f"DuckDB database created successfully at {temp_db_file}")
        return temp_db_file

    except Exception as e:
        logger.error(f"Error creating DuckDB database: {e}")
        if os.path.exists(temp_db_file):
            try:
                os.remove(temp_db_file)
            except:
                pass
        return None


def main():
    parser = argparse.ArgumentParser(
        description="Optimized gene clustering script.")
    parser.add_argument(
        "-i",
        "--input_dir",
        type=str,
        required=True,
        help="Directory containing input CSV files.",
    )
    parser.add_argument(
        "-o",
        "--output_dir",
        type=str,
        required=True,
        help="Directory to store output files.",
    )
    parser.add_argument(
        "-d",
        "--duckdb",
        type=str,
        required=False,
        help="Path to duckdb if it exists already.",
    )
    parser.add_argument("--epsilon",
                        type=float,
                        default=800,
                        help="Epsilon value for DBSCAN1D.")
    parser.add_argument("--min_samples",
                        type=int,
                        default=75,
                        help="Minimum samples value for DBSCAN1D.")
    parser.add_argument(
        "-j",
        "--threads",
        type=int,
        default=4,
        help="Number of threads to use for parallel gene processing.")
    parser.add_argument(
        "--remove_outliers",
        action="store_true",
        help="Whether to remove or keep outliers in plots.",
    )
    parser.add_argument("--batch_size",
                        type=int,
                        default=1000,
                        help="Number of genes to process in each batch.")
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    # Set up logging
    logger.remove()
    if args.verbose == 0:
        logger.add(sys.stderr, level="WARNING")
    elif args.verbose == 1:
        logger.add(sys.stderr, level="INFO")
    else:
        logger.add(sys.stderr, level="DEBUG")

    # Create output directories
    # os.makedirs(os.path.join(args.output_dir, "all_gene_dists"), exist_ok=True)
    passinggenesdir = os.path.join(args.output_dir, "passing_genes")
    os.makedirs(passinggenesdir, exist_ok=True)

    # Create or use existing DuckDB database
    db_file = create_duckdb_with_index(args.input_dir, args.output_dir,
                                       args.duckdb)
    if not db_file:
        logger.error("Failed to create or access DuckDB database. Exiting.")
        sys.exit(1)

    # Generate arguments for gene processing
    args_generator = generate_process_gene_args(db_file, args.epsilon,
                                                args.min_samples,
                                                passinggenesdir,
                                                args.remove_outliers)

    # Process genes in batches
    total_processed, total_passing = batch_process_genes(
        args_generator, args.threads, args.batch_size)

    # Report results
    if total_processed == 0:
        logger.info("No genes were processed.")
    else:
        passing_percentage = (total_passing / total_processed) * 100
        failing_percentage = (
            (total_processed - total_passing) / total_processed) * 100
        logger.info(
            f"Passing genes: {total_passing} ({passing_percentage:.2f}%)\n"
            f"Failing genes: {total_processed - total_passing} ({failing_percentage:.2f}%)"
        )

    logger.success("Processing complete.")

    # Don't remove the database by default - it could be reused
    logger.info(f"Database file kept at: {db_file}")
    logger.info(f"To delete the database file manually: rm {db_file}")


if __name__ == "__main__":
    main()
