#!/usr/bin/env python3

import argparse
import pandas as pd
import os
from loguru import logger


def load_data_in_chunks(file_path, chunksize=50000):
    """
    Load data from a file in chunks to reduce memory usage.

    Args:
        file_path: Path to the input data file
        chunksize: Number of rows per chunk

    Returns:
        Iterator of DataFrame chunks
    """
    try:
        # Create a chunk iterator instead of loading all at once
        chunks = pd.read_csv(file_path, sep="\t", dtype=str, chunksize=chunksize)
        logger.info("Data chunk iterator initialized from {}", file_path)
        return chunks
    except Exception as e:
        logger.error("Failed to load data: {}", e)
        raise


def optimize_dtypes(df):
    """
    Optimize DataFrame memory usage by using appropriate data types.

    Args:
        df: DataFrame to optimize

    Returns:
        Optimized DataFrame
    """
    # Integer columns list
    int_columns = ["Difference", "Distance", "PrefixLocation", "SuffixLocation"]

    # Convert object columns that should be categorical
    for col in df.select_dtypes(include=["object"]).columns:
        if df[col].nunique() < len(df) * 0.5:  # If fewer than 50% unique values
            df[col] = df[col].astype("category")

    # Optimize numeric columns - use integers for specified columns
    for col in df.select_dtypes(include=["float"]).columns:
        if col in int_columns and col in df.columns:
            df[col] = df[col].astype("int64")  # Convert to integer
        else:
            df[col] = pd.to_numeric(df[col], downcast="float")

    for col in df.select_dtypes(include=["int"]).columns:
        df[col] = pd.to_numeric(df[col], downcast="integer")

    return df


def preprocess_chunk(chunk):
    """
    Preprocess a DataFrame chunk: Convert numeric columns and filter non-NaN rows.

    Args:
        chunk: DataFrame chunk to process

    Returns:
        Processed DataFrame chunk
    """
    logger.debug("Preprocessing chunk with {} rows", len(chunk))

    # Convert numeric columns with potential integers
    int_columns = ["Difference", "Distance", "PrefixLocation", "SuffixLocation"]

    # Convert Difference to numeric first (required for filtering)
    chunk["Difference"] = pd.to_numeric(chunk["Difference"], errors="coerce")

    # Filter rows based on Difference not being NaN
    filtered_chunk = chunk[chunk["Difference"].notna()].copy()

    if not filtered_chunk.empty:
        # Convert all potential integer columns
        for col in int_columns:
            if col in filtered_chunk.columns:
                # Convert to numeric first in case they contain non-numeric values
                filtered_chunk[col] = pd.to_numeric(
                    filtered_chunk[col], errors="coerce"
                )
                # Then convert to integer for those that are not NaN
                mask = filtered_chunk[col].notna()
                if mask.any():
                    filtered_chunk.loc[mask, col] = filtered_chunk.loc[
                        mask, col
                    ].astype("int64")

        # Extract GenomeID from Contig
        filtered_chunk["GenomeID"] = filtered_chunk["Contig"].str.split(".").str[0]

        # Optimize memory usage
        filtered_chunk = optimize_dtypes(filtered_chunk)

    logger.debug("Chunk preprocessing complete, {} rows remaining", len(filtered_chunk))
    return filtered_chunk


def find_closest_differences_by_group(filtered_chunks):
    """
    Process chunks to find rows with 'Difference' closest to 0 for each GenomeID and GeneID pair.

    Args:
        filtered_chunks: List of preprocessed DataFrame chunks

    Returns:
        DataFrame with closest differences
    """
    logger.info("Finding closest differences across all chunks")

    # Store results by genome-gene pair to avoid duplicate processing
    closest_results = {}

    for i, chunk in enumerate(filtered_chunks):
        if chunk.empty:
            continue

        logger.debug("Processing chunk {} for closest differences", i + 1)

        # Calculate absolute difference
        chunk["AbsDifference"] = chunk["Difference"].abs()

        # Process each genome-gene pair in this chunk
        for (genome_id, gene_id), group in chunk.groupby(["GenomeID", "GeneID"]):
            min_idx = group["AbsDifference"].idxmin()
            min_row = chunk.loc[min_idx].drop(labels=["AbsDifference"])

            # Only keep if it's better than what we've seen before
            pair_key = (genome_id, gene_id)
            if (
                pair_key not in closest_results
                or min_row["AbsDifference"] < closest_results[pair_key]["AbsDifference"]
            ):
                closest_results[pair_key] = min_row.to_dict()

        # Release memory
        del chunk

    # Convert results dictionary to DataFrame
    if closest_results:
        closest_df = pd.DataFrame(closest_results.values())

        # Ensure integer columns are stored as integers
        int_columns = ["Difference", "Distance", "PrefixLocation", "SuffixLocation"]
        for col in int_columns:
            if col in closest_df.columns:
                closest_df[col] = closest_df[col].astype("int64")

        logger.info(
            "Found {} unique genome-gene pairs with closest differences",
            len(closest_df),
        )
        return closest_df
    else:
        logger.warning("No closest differences found in any chunk")
        return pd.DataFrame()


def process_in_batches(file_path, output_file, chunksize=50000, max_chunks_in_memory=5):
    """
    Process file in chunks with controlled memory usage.

    Args:
        file_path: Path to input file
        output_file: Path to output file
        chunksize: Number of rows to process at once
        max_chunks_in_memory: Maximum number of chunks to keep in memory
    """
    chunks_iterator = load_data_in_chunks(file_path, chunksize)

    # Process chunks in batches to control memory usage
    batch_num = 0
    temp_files = []

    for batch_chunks in iter(
        lambda: list(islice(chunks_iterator, max_chunks_in_memory)), []
    ):
        if not batch_chunks:
            break

        batch_num += 1
        logger.info("Processing batch {} with {} chunks", batch_num, len(batch_chunks))

        # Preprocess all chunks in this batch
        filtered_chunks = [preprocess_chunk(chunk) for chunk in batch_chunks]

        # Find closest differences for this batch
        closest_batch = find_closest_differences_by_group(filtered_chunks)

        if not closest_batch.empty:
            # Write to temporary file
            temp_file = f"temp_batch_{batch_num}.csv"
            closest_batch.to_csv(temp_file, index=False)
            temp_files.append(temp_file)
            logger.info("Saved batch {} results to temporary file", batch_num)

        # Clear memory
        del filtered_chunks
        del closest_batch

    # Merge all temporary files
    merge_temp_files(temp_files, output_file)

    # Clean up temporary files
    for temp_file in temp_files:
        if os.path.exists(temp_file):
            os.remove(temp_file)

    logger.success("File saved to {}", output_file)


def merge_temp_files(temp_files, output_file):
    """
    Merge temporary batch files into final output file.

    Args:
        temp_files: List of temporary file paths
        output_file: Path to final output file
    """
    if not temp_files:
        logger.warning("No temporary files to merge")
        return

    logger.info("Merging {} temporary files into final output", len(temp_files))

    # Process the first file
    combined_results = {}

    # List of columns to ensure are integers
    int_columns = ["Difference", "Distance", "PrefixLocation", "SuffixLocation"]

    for temp_file in temp_files:
        temp_df = pd.read_csv(temp_file)

        # Ensure integer columns are integers when reading from CSV
        for col in int_columns:
            if col in temp_df.columns:
                temp_df[col] = temp_df[col].astype("int64")

        for _, row in temp_df.iterrows():
            genome_id = row["GenomeID"]
            gene_id = row["GeneID"]
            pair_key = (genome_id, gene_id)

            # Only keep if it's better than what we've seen before
            if pair_key not in combined_results or abs(row["Difference"]) < abs(
                combined_results[pair_key]["Difference"]
            ):
                combined_results[pair_key] = row.to_dict()

    # Convert results dictionary to DataFrame and save
    final_df = pd.DataFrame(combined_results.values())

    # Final type check before saving
    for col in int_columns:
        if col in final_df.columns:
            final_df[col] = final_df[col].astype("int64")

    logger.info("Final dataframe has {} rows", len(final_df))

    # Save to output file
    final_df.to_csv(output_file, index=False)


def main():
    """
    Main function to run the memory-optimized analysis.
    """
    parser = argparse.ArgumentParser(
        description="Memory-optimized analysis of genomic data for closest differences."
    )
    parser.add_argument("file_path", help="Path to the genomic data file")
    parser.add_argument("output", help="Path to the output dataframe file")
    parser.add_argument(
        "--chunksize",
        type=int,
        default=50000,
        help="Number of rows to process at once (default: 50000)",
    )
    parser.add_argument(
        "--max-chunks",
        type=int,
        default=5,
        help="Maximum number of chunks to keep in memory (default: 5)",
    )

    args = parser.parse_args()

    # Ensure the output directory exists
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Import here to avoid importing if not used
    global islice
    from itertools import islice

    # Process the file in memory-efficient batches
    process_in_batches(args.file_path, args.output, args.chunksize, args.max_chunks)


if __name__ == "__main__":
    main()
