#!/usr/bin/env python

import scipy
import sys
import matplotlib.ticker as ticker
import pandas as pd
import numpy as np
import re
import glob
import os
import argparse
import dask.dataframe as dd
from loguru import logger
from sklearn.cluster import DBSCAN, OPTICS
from dbscan1d.core import DBSCAN1D
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.collections import PathCollection
from multiprocessing import Pool
import concurrent.futures


def read_and_filter_csv(file):
    df = pd.read_csv(file)
    # Filter out columns that are entirely NA
    filtered_df = df.dropna(axis=1, how='all')
    return filtered_df


def process_file(f):
    indiv_data = read_and_filter_csv(f)
    basename_f = os.path.basename(f)
    bucket_f = re.sub(r"(_\d+)-filterdists\.csv", "", basename_f)
    indiv_data['Bucket'] = bucket_f
    logger.debug(f"Read in {bucket_f}")
    return indiv_data


def aggregate_results_dask(folder_path):
    # Create a glob pattern to match all CSV files in the folder
    csv_pattern = os.path.join(folder_path, "*.csv")

    # Read all CSV files into a Dask DataFrame
    df = dd.read_csv(csv_pattern,
                     assume_missing=True,
                     include_path_column=True)

    # Extract the base filename from the 'path' column
    df['Bucket'] = df['path'].map(lambda x: os.path.basename(x),
                                  meta=('Bucket', 'object'))

    # Use Dask's string methods to extract the 'Bucket' information
    df['Bucket'] = df['Bucket'].str.replace(r"(_\d+)-filterdists\.csv$",
                                            "",
                                            regex=True)

    # Optionally drop the 'path' column if it's no longer needed
    df = df.drop(columns=['path'])

    # Filter out columns that are entirely NA
    # df = df.dropna(axis=1, how='all')

    # Compute the Dask DataFrame to get a pandas DataFrame
    df = df.compute()

    return df


def aggregate_results(folder_path):
    all_files = glob.glob(os.path.join(folder_path, "*.csv"))

    # Use a multiprocessing Pool to process files in parallel
    with Pool() as pool:
        dataframes = pool.map(process_file, all_files)

    # Concatenate all DataFrames at once
    df = pd.concat(dataframes, ignore_index=True)
    return df


def assign_cluster_colors(labels):
    # Sort unique labels, ensuring -1 is last
    unique_labels = sorted(set(labels), key=lambda x: (x == -1, x))
    logger.debug(f"Sorted Labels: {unique_labels}")

    # Get colors from the seaborn muted palette
    n_colors = len(unique_labels)
    logger.debug(f"Number of colors: {n_colors}")

    # Generate a color palette with enough distinct colors
    base_palette = sns.color_palette("muted", as_cmap=False).as_hex()
    if n_colors > len(base_palette):
        logger.warning(
            f"{n_colors} is more than in the seaborn palette. Adding new colors with different brightness."
        )
        extended_palette = base_palette + sns.color_palette(
            "muted", n_colors - len(base_palette), desat=0.6).as_hex()
    else:
        extended_palette = base_palette

    colors = extended_palette[:n_colors]
    logger.debug(f"Available colors: {colors}")

    # Initialize label_color_map with regular clusters
    label_color_map = {
        label: colors[i]
        for i, label in enumerate(unique_labels) if label != -1
    }

    # If -1 is in labels, assign it the last color from the palette
    if -1 in unique_labels:
        label_color_map[-1] = colors[
            -1]  # Assigning the last color in the palette to -1

    return label_color_map


# Function to process a single gene
def process_gene(gene,
                 gene_data,
                 epsilon,
                 min_samples,
                 output_dir,
                 remove_outliers=False):
    logger.debug(f"Processing gene: {gene}")
    gene_name = gene
    if '/' in gene_name:
        logger.warning(f"{gene_name} contains '/', replacing '/' with ':'")
        gene_name = gene_name.replace('/', ':')
        logger.warning(f"{gene_name} replaces {gene}")
    if '|' in gene_name:
        logger.warning(f"{gene_name} contains '|', replacing '|' with ':'")
        gene_name = gene_name.replace('|', ':')
        logger.warning(f"{gene_name} replaces {gene}")
    if '(' in gene_name:
        logger.warning(f"{gene_name} contains '(', replacing with ':'")
        gene_name = gene_name.replace('(', ':')
        logger.warning(f"{gene_name} replaces {gene}")
    if ')' in gene_name:
        logger.warning(f"{gene_name} contains ')', replacing with ':'")
        gene_name = gene_name.replace(')', ':')
        logger.warning(f"{gene_name} replaces {gene}")
    if "'" in gene_name:
        logger.warning(f"{gene_name} contains \"'\", replacting with ':'")
        gene_name = gene_name.replace("'", ":")

    # Count the occurrences of each unique value for sample_weight
    value_counts = gene_data["Difference"].value_counts()
    # Remove exact duplicates
    gene_data_unique = gene_data.drop_duplicates(subset=["Difference"])
    # Reshape for DBSCAN
    differences = gene_data_unique["Difference"].values.reshape(-1, 1)

    # for OPTICS
    # differences = gene_data['Difference'].values.reshape(-1, 1)

    create_plot(
        gene_data, gene_name,
        os.path.join(output_dir, "all_gene_dists",
                     f"{gene_name}_distances.png"))

    # logger.debug(differences.shape)
    # if min_samples > differences.shape[0]:
    # logger.warning(f"Min. samples ({min_samples}) greater than number of samples ({differences.shape})... Skipping {gene_name}")
    # return gene, None, False, None

    # create_violin_plot(
    # gene_data, gene_name,
    # os.path.join(output_dir, "all_differencedist_plots",
    # f"{gene_name}_dist.png"))

    logger.debug(f"Clustering differences")
    # dbscan = DBSCAN(eps=epsilon, min_samples=min_samples, n_jobs=4)
    dbscan = DBSCAN1D(eps=epsilon, min_samples=min_samples)
    # optics = OPTICS(max_eps=epsilon,
    # cluster_method='dbscan',
    # min_samples=min_samples,
    # xi=0.3,
    # n_jobs=-1,
    # metric='manhattan')

    logger.debug(f"Fitting labels")

    # Use sample_weight in DBSCAN
    sample_weight = value_counts.loc[gene_data_unique["Difference"]].values
    labels = dbscan.fit_predict(differences, sample_weight=sample_weight)
    # labels = optics.fit_predict(differences)

    logger.debug(f"Checking gene labels")

    # Check if the gene meets the criteria
    label_set = set(labels)
    if (-1 in label_set and len(label_set) <= 2) or len(label_set) < 2:
        return gene, None, False, None

    logger.info(
        f"{gene} passed filter with the following clusters: {label_set}")

    # Compute mean distance for each cluster except noise (-1)
    cluster_means = {}
    for label in label_set:
        if label != -1:
            cluster_members = differences[labels == label]
            mean_distance = np.mean(cluster_members)
            cluster_means[label] = mean_distance

    # Sort clusters based on their mean distance and assign new labels
    sorted_clusters = sorted(cluster_means, key=cluster_means.get)
    cluster_mapping = {
        old_label: new_label
        for new_label, old_label in enumerate(sorted_clusters)
    }
    # print(cluster_mapping)

    # Replace old labels with new labels
    new_labels = np.array(
        [cluster_mapping[label] if label != -1 else -1 for label in labels])
    # print(new_labels)

    # Create a mapping from unique values to labels
    mapping = dict(zip(gene_data_unique["Difference"], new_labels))
    # mapping = dict(zip(gene_data['Difference'], new_labels))
    # print(mapping)

    gene_df = gene_data.copy()
    gene_df["Cluster"] = gene_df['Difference'].map(mapping)
    # print(pd.unique(gene_df['Cluster']))
    outdir = os.path.join(output_dir, "passing_genes", gene_name)
    os.makedirs(outdir, exist_ok=True)

    # Plotting and saving
    if remove_outliers:
        create_plot(gene_df.loc[gene_df['Cluster'] != -1], gene_name,
                    os.path.join(outdir, f"{gene_name}_clusters.png"))
    else:
        create_plot(gene_df, gene_name,
                    os.path.join(outdir, f"{gene_name}_clusters.png"))
    gene_df.to_csv(os.path.join(outdir, f"{gene_name}_clusters.csv"),
                   index=False)

    return gene, mapping, True, gene_name


# Function to perform clustering and save results
def cluster_gene_differences_threaded(df, epsilon, min_samples, output_dir,
                                      remove_outliers):
    if len(df) == 0:
        return 0, 0

    grouped_data = df.groupby('GeneID')

    num_failing_genes = 0
    num_passing_genes = 0
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = {}
        for gene, gene_data in grouped_data:
            futures[executor.submit(process_gene, gene, gene_data, epsilon,
                                    min_samples, output_dir,
                                    remove_outliers)] = gene

        for future in concurrent.futures.as_completed(futures):
            gene, mapping, is_worthy, gene_name = future.result()

            if is_worthy:
                num_passing_genes += 1
            else:
                num_failing_genes += 1
    return num_passing_genes, num_failing_genes


def create_plot(big_df, gene_id, output):
    big_df = big_df.copy()
    if 'Cluster' in big_df.columns:
        # print(pd.unique(big_df['Cluster']))
        # print(big_df['Cluster'].isnull().sum())
        palette_ints = assign_cluster_colors(big_df['Cluster'].astype(int))
        # print(palette_ints)
        big_df['Cluster'] = big_df['Cluster'].astype(str)
        # print(big_df['Cluster'])
        palette = {}
        for i in palette_ints:
            palette[str(i)] = palette_ints[i]
    # Create a figure with two subplots (rows), sharing the same x-axis
    fig, axs = plt.subplots(2,
                            1,
                            figsize=(5, 4),
                            sharex=True,
                            gridspec_kw={'height_ratios': [3, 1]})

    # First plot (KDE) on the first subplot
    sns.histplot(data=big_df,
                 x="Difference",
                 ax=axs[0],
                 fill=False,
                 color='black')
    axs[0].set_ylabel("Number of Genomes")
    axs[0].set_title(f"{gene_id}")
    # Remove x-axis label for the top plot to only have it at the bottom plot
    axs[0].set_xlabel("")
    axs[0].yaxis.set_major_formatter(
        ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x)
                             if x == 0 else '{:,.3g}'.format(x / 1000) + 'K'))

    if 'Cluster' in big_df.columns:
        # Second plot (Stripplot and Violinplot) on the second subplot
        sns.stripplot(x='Difference',
                      data=big_df.sort_values(
                          'Cluster', key=lambda x: x.replace('-1', 'A')),
                      hue='Cluster',
                      orient="h",
                      ax=axs[1],
                      alpha=0.8,
                      palette=palette)
    else:
        sns.stripplot(
            x='Difference',
            data=big_df,
            color='black',
            orient="h",
            ax=axs[1],
            alpha=0.8,
        )

    sns.violinplot(x='Difference',
                   data=big_df,
                   fill=True,
                   inner=None,
                   cut=0,
                   color='lightgrey',
                   alpha=0.8,
                   split=False,
                   ax=axs[1])

    if "Cluster" in big_df.columns:
        handles, labels = axs[1].get_legend_handles_labels()
        fig.legend(handles,
                   labels,
                   title="Cluster",
                   bbox_transform=fig.transFigure)
        axs[1].get_legend().remove()

    # Set labels for the second plot
    axs[1].set_xlabel("Prefix-suffix distance relative to gene length")
    axs[1].xaxis.set_major_formatter(
        ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x)
                             if x == 0 else '{:,.3g}'.format(x / 1000) + 'kb'))
    axs[1].set(ylabel=None)  # Remove y-axis label
    axs[1].set_yticks([])
    sns.despine()
    sns.despine(left=True, ax=axs[1])

    # Adjust layout to not overlap and save the figure
    plt.tight_layout()

    plt.savefig(output, dpi=300, transparent=True)
    plt.close()


def main():
    parser = argparse.ArgumentParser(description="Gene clustering script.")
    parser.add_argument(
        "-i",
        "--input_dir",
        type=str,
        required=True,
        help="Directory containing input CSV files.",
    )
    parser.add_argument(
        "-o",
        "--output_dir",
        type=str,
        required=True,
        help="Directory to store output files.",
    )
    parser.add_argument("--epsilon",
                        type=float,
                        default=800,
                        help="Epsilon value for OPTICS.")
    parser.add_argument("--min_samples",
                        type=int,
                        default=75,
                        help="Minimum samples value for OPTICS.")
    parser.add_argument("--remove_outliers",
                        action="store_true",
                        help="Whether to remove or keep outliers in plots.")
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    if args.verbose == 0:
        logger.remove()
    elif args.verbose == 1:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    elif args.verbose >= 2:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    os.makedirs(os.path.join(args.output_dir, "all_gene_dists"), exist_ok=True)
    passinggenesdir = os.path.join(args.output_dir, "passing_genes")
    os.makedirs(passinggenesdir, exist_ok=True)
    logger.debug("Aggregating inputs")
    # df = aggregate_results(args.input_dir)
    df = aggregate_results_dask(args.input_dir)
    logger.info("Read all input files.")
    # print(df)
    logger.info("Read all input files.")
    (num_passing_genes, num_failing_genes) = cluster_gene_differences_threaded(
        df, args.epsilon, args.min_samples, args.output_dir,
        args.remove_outliers)
    if num_passing_genes + num_failing_genes == 0:
        logger.info("No genes passed filters.")
    else:
        logger.info(
            f"Passing genes: {num_passing_genes} ({(num_passing_genes/(num_passing_genes+num_failing_genes))*100}%)\nFailing genes: {num_failing_genes} ({(num_failing_genes/(num_passing_genes+num_failing_genes))*100}%)"
        )
    logger.success("Complete.")


if __name__ == "__main__":
    main()
