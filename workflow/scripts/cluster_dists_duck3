#!/usr/bin/env python

import duckdb
import scipy
import gzip
from loguru import logger
import matplotlib.pyplot as plt
import os
import re
import uuid
from dbscan1d.core import DBSCAN1D
import seaborn as sns
from matplotlib.collections import PathCollection
import glob
import pandas as pd
import sys
import argparse
import tempfile
import numpy as np
import concurrent.futures
import matplotlib.ticker as ticker
import gc


def process_gene(args):
    gene_id, epsilon, min_samples, output_dir, remove_outliers, db_file = args

    # Set up logging in the child process
    logger.remove()
    logger.add(sys.stderr, level="INFO")

    try:
        # Connect to the persistent DuckDB database
        con = duckdb.connect(database=db_file, read_only=True)

        # Use a parameterized query to safely include gene_id
        query = "SELECT * FROM gene_data WHERE GeneID = ?"
        df = con.execute(query, [gene_id]).df()
        con.close()  # Close connection immediately after use

        if df.empty:
            # No data for this gene
            return gene_id, None, False, None

        # Clean gene_name for file system usage
        gene_name = gene_id.replace("/", ":").replace("|", ":").replace(
            "(", ":").replace(")", ":").replace("'", ":")

        # Optimize the dataframe by selecting only needed columns
        # needed_columns = ['Difference', 'filename']
        # if set(needed_columns).issubset(df.columns):
        # df = df[needed_columns].copy()

        df['Bucket'] = df['filename'].apply(lambda x: re.sub(
            r'-filterdists\.csv\.gz$', '', os.path.basename(x)))

        # Ensure 'Difference' column is numeric
        df['Difference'] = pd.to_numeric(df['Difference'], errors='coerce')

        # Drop rows with NaN 'Difference' values
        df = df.dropna(subset=['Difference'])

        if df.empty:
            logger.info(
                f"GeneID {gene_id} has no valid 'Difference' data after cleaning."
            )
            return gene_id, None, False, None

        # Count the occurrences of each unique value for sample_weight
        value_counts = df['Difference'].value_counts()

        # Remove exact duplicates
        gene_data_unique = df.drop_duplicates(subset=["Difference"])

        # Reshape for DBSCAN
        differences = gene_data_unique["Difference"].values.reshape(-1, 1)

        # Use sample_weight in DBSCAN
        sample_weight = value_counts.loc[gene_data_unique["Difference"]].values

        # Perform clustering
        dbscan = DBSCAN1D(eps=epsilon, min_samples=min_samples)
        labels = dbscan.fit_predict(differences, sample_weight=sample_weight)

        # Check if the gene meets the criteria
        label_set = set(labels)
        if (-1 in label_set and len(label_set) <= 2) or len(label_set) < 2:
            logger.info(
                f"GeneID {gene_id} did not pass the clustering criteria.")
            return gene_id, None, False, None

        logger.info(f"GeneID {gene_id} passed the clustering criteria.")

        # Compute mean distance for each cluster except noise (-1)
        cluster_means = {}
        for label in label_set:
            if label != -1:
                cluster_members = differences[labels == label]
                mean_distance = np.mean(cluster_members)
                cluster_means[label] = mean_distance

        # Sort clusters based on their mean distance and assign new labels
        sorted_clusters = sorted(cluster_means, key=cluster_means.get)
        cluster_mapping = {
            old_label: new_label
            for new_label, old_label in enumerate(sorted_clusters)
        }

        # Replace old labels with new labels
        new_labels = np.array([
            cluster_mapping[label] if label != -1 else -1 for label in labels
        ])

        # Create a mapping from unique values to labels
        mapping = dict(zip(gene_data_unique["Difference"], new_labels))

        # Add 'Cluster' column to df
        df["Cluster"] = df["Difference"].map(mapping)

        outdir = os.path.join(output_dir, "passing_genes", gene_name)
        os.makedirs(outdir, exist_ok=True)

        # Plotting and saving
        if remove_outliers:
            plot_data = df.loc[df["Cluster"] != -1]
        else:
            plot_data = df

        create_plot(
            plot_data,
            gene_name,
            os.path.join(outdir, f"{gene_name}_clusters.png"),
        )

        # Save the DataFrame to CSV
        df.to_csv(os.path.join(outdir, f"{gene_name}_clusters.csv.gz"),
                  index=False,
                  compression='gzip')

        # Help garbage collection
        del df
        del plot_data
        gc.collect()

        return gene_id, mapping, True, gene_name

    except Exception as e:
        logger.error(f"Error processing gene {gene_id}: {e}")
        return gene_id, None, False, None


def create_plot(big_df, gene_id, output):
    # Optimize memory usage by creating a smaller dataframe with just what we need
    try:
        # Create a copy of only the necessary columns to reduce memory usage
        plot_df = big_df[[
            "Difference", "Cluster"
        ]].copy() if "Cluster" in big_df.columns else big_df[["Difference"
                                                              ]].copy()

        if "Cluster" in plot_df.columns:
            palette_ints = assign_cluster_colors(
                plot_df["Cluster"].astype(int))
            plot_df.loc[:, "Cluster"] = plot_df["Cluster"].astype(str)
            palette = {str(i): palette_ints[i] for i in palette_ints}

        # Set figure DPI lower for better memory usage
        fig, axs = plt.subplots(2,
                                1,
                                figsize=(5, 4),
                                sharex=True,
                                gridspec_kw={"height_ratios": [3, 1]})

        # Use bins to reduce memory requirements for large datasets
        bins = min(100, max(20, int(len(plot_df) / 100)))  # Adaptive binning
        sns.histplot(data=plot_df,
                     x="Difference",
                     ax=axs[0],
                     bins=bins,
                     fill=False,
                     color="black")

        axs[0].set_yscale('log')
        axs[0].set_ylabel("Number of Genomes")
        axs[0].set_title(f"{gene_id}")
        axs[0].set_xlabel("")
        axs[0].yaxis.set_major_formatter(
            ticker.FuncFormatter(lambda x, pos: "{:,.0f}".format(x) if x == 0
                                 else "{:,.3g}".format(x / 1000) + "K"))

        # For very large datasets, sample the data to reduce memory usage in plots
        # sample_size = min(5000, len(plot_df))
        sample_size = len(plot_df)
        if len(plot_df) > sample_size:
            plot_sample = plot_df.sample(sample_size, random_state=42)
        else:
            plot_sample = plot_df

        if "Cluster" in plot_df.columns:
            # Second plot (Stripplot)
            sns.stripplot(
                x="Difference",
                data=plot_sample.sort_values(
                    "Cluster", key=lambda x: x.replace("-1", "A")),
                hue="Cluster",
                orient="h",
                ax=axs[1],
                alpha=0.8,
                palette=palette,
                jitter=True,
                size=3  # Smaller point size for better performance
            )
        else:
            sns.stripplot(x="Difference",
                          data=plot_sample,
                          color="black",
                          orient="h",
                          ax=axs[1],
                          alpha=0.8,
                          jitter=True,
                          size=3)

        # For violin plots, use a smaller sample if dataset is very large
        violin_sample = plot_df
        if len(plot_df) > 10000:
            violin_sample = plot_df.sample(10000, random_state=42)

        sns.violinplot(
            x="Difference",
            data=violin_sample,
            fill=True,
            inner=None,
            cut=0,
            color="lightgrey",
            alpha=0.8,
            split=False,
            ax=axs[1],
        )

        if "Cluster" in plot_df.columns:
            handles, labels = axs[1].get_legend_handles_labels()
            fig.legend(handles,
                       labels,
                       title="Cluster",
                       bbox_transform=fig.transFigure)
            if axs[1].get_legend() is not None:
                axs[1].get_legend().remove()

        # Set labels for the second plot
        axs[1].set_xlabel("Prefix-suffix distance relative to gene length")
        axs[1].xaxis.set_major_formatter(
            ticker.FuncFormatter(lambda x, pos: "{:,.0f}".format(x) if x == 0
                                 else "{:,.3g}".format(x / 1000) + "kb"))
        axs[1].set(ylabel=None)  # Remove y-axis label
        axs[1].set_yticks([])
        sns.despine()
        sns.despine(left=True, ax=axs[1])

        # Adjust layout and save the figure
        plt.tight_layout()
        plt.savefig(output, dpi=300, transparent=True)
        plt.close(fig)  # Explicitly close the figure

        # Help garbage collection
        del plot_df
        del plot_sample
        if 'violin_sample' in locals():
            del violin_sample
        gc.collect()

    except Exception as e:
        logger.error(f"Error creating plot for {gene_id}: {e}")
        plt.close('all')  # Close any open figures in case of error


def assign_cluster_colors(labels):
    # Sort unique labels, ensuring -1 is last
    unique_labels = sorted(set(labels), key=lambda x: (x == -1, x))

    # Get colors from the seaborn muted palette
    n_colors = len(unique_labels)

    # Generate a color palette with enough distinct colors
    base_palette = sns.color_palette("muted", as_cmap=False).as_hex()
    if n_colors > len(base_palette):
        extended_palette = (base_palette + sns.color_palette(
            "muted", n_colors - len(base_palette), desat=0.6).as_hex())
    else:
        extended_palette = base_palette

    colors = extended_palette[:n_colors]

    # Initialize label_color_map with regular clusters
    label_color_map = {
        label: colors[i]
        for i, label in enumerate(unique_labels) if label != -1
    }

    # If -1 is in labels, assign it the last color from the palette
    if -1 in unique_labels:
        label_color_map[-1] = colors[-1]

    return label_color_map


def generate_process_gene_args(db_path, epsilon_val, min_samples_val,
                               output_dir_val, remove_outliers_val):
    """
    Generator function to yield arguments for process_gene.
    It streams GeneIDs from the database in batches to minimize memory usage.
    """
    try:
        logger.debug(f"Generator connecting to DB: {db_path}")
        conn = duckdb.connect(database=db_path, read_only=True)

        # Use a more efficient query to get distinct GeneIDs
        # This might be more efficient with a direct SQL approach
        query = """
        SELECT DISTINCT GeneID FROM gene_data
        ORDER BY GeneID  -- Optional: ordering makes processing more predictable
        """

        # Check that the table exists
        try:
            conn.execute("SELECT 1 FROM gene_data LIMIT 1")
            logger.debug("gene_data table confirmed to exist by generator.")
        except duckdb.Error as e:
            logger.error(f"Generator: 'gene_data' table not found: {e}")
            conn.close()
            return  # abort generator early

        # Use a DB-API cursor for streaming
        cursor = conn.cursor()
        cursor.execute(query)

        # Larger batch size for better DB performance
        batch_size = 1000
        gene_count = 0

        while True:
            rows = cursor.fetchmany(batch_size)
            if not rows:
                break

            for row in rows:
                gene_id = row[0]
                gene_count += 1
                yield (gene_id, epsilon_val, min_samples_val, output_dir_val,
                       remove_outliers_val, db_path)

        logger.info(f"Generator: Finished yielding {gene_count} GeneIDs.")
        conn.close()

    except Exception as e:
        logger.error(f"Exception in generate_process_gene_args: {e}")
        if 'conn' in locals() and conn:
            conn.close()


def batch_process_genes(args_generator, num_threads, batch_size=10000):
    """
    Process genes in batches to limit memory usage.
    """
    logger.info(f"Starting batch processing with {num_threads} threads")

    total_processed = 0
    total_passing = 0

    try:
        # Process genes in batches
        batch_args = []
        for i, args in enumerate(args_generator):
            batch_args.append(args)

            # When we reach batch size, process the batch
            if len(batch_args) >= batch_size:
                logger.info(f"Processing batch of {len(batch_args)} genes")
                batch_results = process_batch(batch_args, num_threads)

                # Count passing genes
                passing_genes = sum(1 for res in batch_results
                                    if res and res[2])
                total_passing += passing_genes
                total_processed += len(batch_args)

                logger.info(
                    f"Batch completed. Genes passed: {passing_genes}/{len(batch_args)}"
                )
                logger.info(
                    f"Total progress: {total_processed} genes processed, {total_passing} passed"
                )

                # Clear batch for next iteration
                batch_args = []
                gc.collect()  # Explicit garbage collection

        # Process any remaining genes
        if batch_args:
            logger.info(f"Processing final batch of {len(batch_args)} genes")
            batch_results = process_batch(batch_args, num_threads)
            passing_genes = sum(1 for res in batch_results if res and res[2])
            total_passing += passing_genes
            total_processed += len(batch_args)

    except Exception as e:
        logger.error(f"Error in batch processing: {e}")

    return total_processed, total_passing


def process_batch(batch_args, num_threads):
    """
    Process a batch of genes using a process pool.
    """
    results = []
    with concurrent.futures.ProcessPoolExecutor(
            max_workers=num_threads) as executor:
        # Use a reasonable chunksize for better performance
        chunksize = max(1, min(len(batch_args) // (num_threads * 4), 10))
        results = list(
            executor.map(process_gene, batch_args, chunksize=chunksize))

    return results


def create_duckdb_with_index(input_dir, output_dir, db_file=None):
    """
    Create a DuckDB database with an index on GeneID for faster queries.
    """
    if db_file and os.path.exists(db_file):
        logger.info(f"Using existing database at {db_file}")
        return db_file

    # Generate a unique database file
    unique_id = str(uuid.uuid4())
    temp_db_file = os.path.join(output_dir, f'gene_data_{unique_id}.duckdb')

    try:
        logger.info(f"Creating new DuckDB database at {temp_db_file}")
        con_write = duckdb.connect(database=temp_db_file, read_only=False)

        # Find all CSV files
        csv_pattern = os.path.join(input_dir, "*.csv.gz")
        csv_files = glob.glob(csv_pattern)

        if not csv_files:
            logger.error("No CSV files found. Aborting.")
            con_write.close()
            return None

        logger.info(f"Found {len(csv_files)} CSV files to process")

        # Check for non-empty files
        non_empty_files = []
        for file in csv_files:
            try:
                with gzip.open(file, 'rt') as f:
                    header = f.readline()
                    first_line = f.readline()
                    if first_line:
                        non_empty_files.append(file)
            except Exception as e:
                logger.warning(f"Error checking file {file}: {e}")

        if not non_empty_files:
            logger.error("No non-empty CSV files found. Aborting.")
            con_write.close()
            return None

        logger.info(f"Processing {len(non_empty_files)} non-empty CSV files")

        # Define column types with minimal memory usage where possible
        column_types = {
            'GenomeID': 'VARCHAR',
            'GeneID': 'VARCHAR',
            'PrefixID': 'VARCHAR',
            'SuffixID': 'VARCHAR',
            'PrefixContig': 'VARCHAR',
            'SuffixContig': 'VARCHAR',
            'Contig': 'VARCHAR',
            'Distance': 'BIGINT',
            'Difference': 'DOUBLE',
            'PrefixLocation': 'BIGINT',
            'SuffixLocation': 'BIGINT',
        }

        # Process files in chunks to save memory
        batch_size = 1000  # Adjust based on file sizes and available memory

        for i in range(0, len(non_empty_files), batch_size):
            batch_files = non_empty_files[i:i + batch_size]
            logger.info(
                f"Processing batch of {len(batch_files)} files ({i+1}-{i+len(batch_files)} of {len(non_empty_files)})"
            )

            try:
                # Read this batch of CSV files
                rel = con_write.read_csv(
                    batch_files,
                    compression='gzip',
                    filename=True,
                    header=True,
                    sep=',',
                    dtype=column_types,
                )

                # For the first batch, create the table; for subsequent batches, append
                if i == 0:
                    rel.create('gene_data')
                else:
                    rel.insert_into('gene_data')

                # Force a checkpoint to write data to disk
                con_write.execute("CHECKPOINT")

            except Exception as e:
                logger.error(
                    f"Error processing batch {i//batch_size + 1}: {e}")

        # Create an index on GeneID for faster queries
        logger.info("Creating index on GeneID column")
        con_write.execute(
            "CREATE INDEX IF NOT EXISTS gene_id_idx ON gene_data(GeneID)")

        # Final checkpoint
        con_write.execute("CHECKPOINT")
        con_write.close()

        logger.info(f"DuckDB database created successfully at {temp_db_file}")
        return temp_db_file

    except Exception as e:
        logger.error(f"Error creating DuckDB database: {e}")
        if os.path.exists(temp_db_file):
            try:
                os.remove(temp_db_file)
            except:
                pass
        return None


def main():
    parser = argparse.ArgumentParser(
        description="Optimized gene clustering script.")
    parser.add_argument(
        "-i",
        "--input_dir",
        type=str,
        required=True,
        help="Directory containing input CSV files.",
    )
    parser.add_argument(
        "-o",
        "--output_dir",
        type=str,
        required=True,
        help="Directory to store output files.",
    )
    parser.add_argument(
        "-d",
        "--duckdb",
        type=str,
        required=False,
        help="Path to duckdb if it exists already.",
    )
    parser.add_argument("--epsilon",
                        type=float,
                        default=800,
                        help="Epsilon value for DBSCAN1D.")
    parser.add_argument("--min_samples",
                        type=int,
                        default=75,
                        help="Minimum samples value for DBSCAN1D.")
    parser.add_argument(
        "-j",
        "--threads",
        type=int,
        default=4,
        help="Number of threads to use for parallel gene processing.")
    parser.add_argument(
        "--remove_outliers",
        action="store_true",
        help="Whether to remove or keep outliers in plots.",
    )
    parser.add_argument("--batch_size",
                        type=int,
                        default=1000,
                        help="Number of genes to process in each batch.")
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help=
        "Increase verbosity level. Can be specified multiple times for more detailed logs.",
    )

    args = parser.parse_args()

    # Set up logging
    logger.remove()
    if args.verbose == 0:
        logger.add(sys.stderr, level="WARNING")
    elif args.verbose == 1:
        logger.add(sys.stderr, level="INFO")
    else:
        logger.add(sys.stderr, level="DEBUG")

    # Create output directories
    os.makedirs(os.path.join(args.output_dir, "all_gene_dists"), exist_ok=True)
    passinggenesdir = os.path.join(args.output_dir, "passing_genes")
    os.makedirs(passinggenesdir, exist_ok=True)

    # Create or use existing DuckDB database
    db_file = create_duckdb_with_index(args.input_dir, args.output_dir,
                                       args.duckdb)
    if not db_file:
        logger.error("Failed to create or access DuckDB database. Exiting.")
        sys.exit(1)

    # Generate arguments for gene processing
    args_generator = generate_process_gene_args(db_file, args.epsilon,
                                                args.min_samples,
                                                args.output_dir,
                                                args.remove_outliers)

    # Process genes in batches
    total_processed, total_passing = batch_process_genes(
        args_generator, args.threads, args.batch_size)

    # Report results
    if total_processed == 0:
        logger.info("No genes were processed.")
    else:
        passing_percentage = (total_passing / total_processed) * 100
        failing_percentage = (
            (total_processed - total_passing) / total_processed) * 100
        logger.info(
            f"Passing genes: {total_passing} ({passing_percentage:.2f}%)\n"
            f"Failing genes: {total_processed - total_passing} ({failing_percentage:.2f}%)"
        )

    logger.success("Processing complete.")

    # Don't remove the database by default - it could be reused
    logger.info(f"Database file kept at: {db_file}")
    logger.info(f"To delete the database file manually: rm {db_file}")


if __name__ == "__main__":
    main()
